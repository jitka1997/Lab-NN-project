{
  "best_global_step": 3912,
  "best_metric": 5.691287040710449,
  "best_model_checkpoint": "/Users/jitkamuravska/Neural-Networks/project/formal-model/checkpoint-3912",
  "epoch": 3.9992337164750955,
  "eval_steps": 500,
  "global_step": 3912,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05108556832694764,
      "grad_norm": 1.866227388381958,
      "learning_rate": 4.9e-05,
      "loss": 8.9722,
      "step": 50
    },
    {
      "epoch": 0.10217113665389528,
      "grad_norm": 1.4961140155792236,
      "learning_rate": 9.900000000000001e-05,
      "loss": 8.4238,
      "step": 100
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 1.367257833480835,
      "learning_rate": 9.997418194579384e-05,
      "loss": 7.6629,
      "step": 150
    },
    {
      "epoch": 0.20434227330779056,
      "grad_norm": 1.3869818449020386,
      "learning_rate": 9.98946373877661e-05,
      "loss": 7.03,
      "step": 200
    },
    {
      "epoch": 0.2554278416347382,
      "grad_norm": 1.2559688091278076,
      "learning_rate": 9.976144110624448e-05,
      "loss": 6.5658,
      "step": 250
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 1.0091583728790283,
      "learning_rate": 9.957473632724365e-05,
      "loss": 6.2062,
      "step": 300
    },
    {
      "epoch": 0.35759897828863346,
      "grad_norm": 0.7068941593170166,
      "learning_rate": 9.933472381448698e-05,
      "loss": 6.0022,
      "step": 350
    },
    {
      "epoch": 0.4086845466155811,
      "grad_norm": 0.8250604271888733,
      "learning_rate": 9.904166165352523e-05,
      "loss": 5.9122,
      "step": 400
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 0.8403750658035278,
      "learning_rate": 9.869586497421697e-05,
      "loss": 5.8161,
      "step": 450
    },
    {
      "epoch": 0.5108556832694764,
      "grad_norm": 0.9811693429946899,
      "learning_rate": 9.829770561186952e-05,
      "loss": 5.7769,
      "step": 500
    },
    {
      "epoch": 0.561941251596424,
      "grad_norm": 0.9554510116577148,
      "learning_rate": 9.784761170740406e-05,
      "loss": 5.6941,
      "step": 550
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 1.0447206497192383,
      "learning_rate": 9.73460672469757e-05,
      "loss": 5.6567,
      "step": 600
    },
    {
      "epoch": 0.6641123882503193,
      "grad_norm": 1.040436029434204,
      "learning_rate": 9.679361154154289e-05,
      "loss": 5.6468,
      "step": 650
    },
    {
      "epoch": 0.7151979565772669,
      "grad_norm": 1.149364948272705,
      "learning_rate": 9.619083864694612e-05,
      "loss": 5.5541,
      "step": 700
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 1.384412407875061,
      "learning_rate": 9.553839672511948e-05,
      "loss": 5.5239,
      "step": 750
    },
    {
      "epoch": 0.8173690932311622,
      "grad_norm": 1.1959803104400635,
      "learning_rate": 9.483698734712189e-05,
      "loss": 5.4739,
      "step": 800
    },
    {
      "epoch": 0.8684546615581098,
      "grad_norm": 1.5442718267440796,
      "learning_rate": 9.408736473873743e-05,
      "loss": 5.4443,
      "step": 850
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 1.5902714729309082,
      "learning_rate": 9.329033496945602e-05,
      "loss": 5.4375,
      "step": 900
    },
    {
      "epoch": 0.9706257982120051,
      "grad_norm": 1.524254322052002,
      "learning_rate": 9.244675508570662e-05,
      "loss": 5.4053,
      "step": 950
    },
    {
      "epoch": 0.9992337164750957,
      "eval_loss": 6.0530781745910645,
      "eval_runtime": 3.8714,
      "eval_samples_per_second": 787.051,
      "eval_steps_per_second": 98.414,
      "step": 978
    },
    {
      "epoch": 1.022477650063857,
      "grad_norm": 1.4473673105239868,
      "learning_rate": 9.155753218927489e-05,
      "loss": 5.4392,
      "step": 1000
    },
    {
      "epoch": 1.0735632183908046,
      "grad_norm": 1.6348706483840942,
      "learning_rate": 9.062362246189618e-05,
      "loss": 5.2834,
      "step": 1050
    },
    {
      "epoch": 1.1246487867177521,
      "grad_norm": 1.713852047920227,
      "learning_rate": 8.964603013707304e-05,
      "loss": 5.2825,
      "step": 1100
    },
    {
      "epoch": 1.1757343550447,
      "grad_norm": 1.6725490093231201,
      "learning_rate": 8.86258064202225e-05,
      "loss": 5.2634,
      "step": 1150
    },
    {
      "epoch": 1.2268199233716475,
      "grad_norm": 1.6562501192092896,
      "learning_rate": 8.756404835831436e-05,
      "loss": 5.2004,
      "step": 1200
    },
    {
      "epoch": 1.2779054916985952,
      "grad_norm": 1.7635730504989624,
      "learning_rate": 8.646189766021631e-05,
      "loss": 5.2052,
      "step": 1250
    },
    {
      "epoch": 1.3289910600255428,
      "grad_norm": 1.767977237701416,
      "learning_rate": 8.532053946901384e-05,
      "loss": 5.1802,
      "step": 1300
    },
    {
      "epoch": 1.3800766283524903,
      "grad_norm": 2.0027849674224854,
      "learning_rate": 8.414120108762544e-05,
      "loss": 5.1822,
      "step": 1350
    },
    {
      "epoch": 1.4311621966794381,
      "grad_norm": 1.9189683198928833,
      "learning_rate": 8.292515065908321e-05,
      "loss": 5.1845,
      "step": 1400
    },
    {
      "epoch": 1.4822477650063857,
      "grad_norm": 2.0639607906341553,
      "learning_rate": 8.167369580289825e-05,
      "loss": 5.1534,
      "step": 1450
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 2.002859592437744,
      "learning_rate": 8.038818220897682e-05,
      "loss": 5.1622,
      "step": 1500
    },
    {
      "epoch": 1.584418901660281,
      "grad_norm": 1.8851312398910522,
      "learning_rate": 7.906999219059945e-05,
      "loss": 5.1057,
      "step": 1550
    },
    {
      "epoch": 1.6355044699872288,
      "grad_norm": 1.909989356994629,
      "learning_rate": 7.772054319801894e-05,
      "loss": 5.0962,
      "step": 1600
    },
    {
      "epoch": 1.686590038314176,
      "grad_norm": 1.9794425964355469,
      "learning_rate": 7.63412862942755e-05,
      "loss": 5.0926,
      "step": 1650
    },
    {
      "epoch": 1.7376756066411239,
      "grad_norm": 1.8993645906448364,
      "learning_rate": 7.493370459486816e-05,
      "loss": 5.0844,
      "step": 1700
    },
    {
      "epoch": 1.7887611749680716,
      "grad_norm": 2.1952502727508545,
      "learning_rate": 7.349931167295996e-05,
      "loss": 5.0861,
      "step": 1750
    },
    {
      "epoch": 1.8398467432950192,
      "grad_norm": 1.9576390981674194,
      "learning_rate": 7.203964993183229e-05,
      "loss": 5.0501,
      "step": 1800
    },
    {
      "epoch": 1.8909323116219667,
      "grad_norm": 2.10573673248291,
      "learning_rate": 7.05562889463378e-05,
      "loss": 5.0562,
      "step": 1850
    },
    {
      "epoch": 1.9420178799489145,
      "grad_norm": 2.2050952911376953,
      "learning_rate": 6.90508237751362e-05,
      "loss": 5.0422,
      "step": 1900
    },
    {
      "epoch": 1.993103448275862,
      "grad_norm": 2.470125198364258,
      "learning_rate": 6.752487324552682e-05,
      "loss": 4.9992,
      "step": 1950
    },
    {
      "epoch": 1.9992337164750957,
      "eval_loss": 5.795938968658447,
      "eval_runtime": 3.8555,
      "eval_samples_per_second": 790.302,
      "eval_steps_per_second": 98.82,
      "step": 1956
    },
    {
      "epoch": 2.044955300127714,
      "grad_norm": 2.101994514465332,
      "learning_rate": 6.59800782127231e-05,
      "loss": 5.0725,
      "step": 2000
    },
    {
      "epoch": 2.0960408684546614,
      "grad_norm": 2.1276628971099854,
      "learning_rate": 6.441809979544027e-05,
      "loss": 4.9268,
      "step": 2050
    },
    {
      "epoch": 2.147126436781609,
      "grad_norm": 2.5082314014434814,
      "learning_rate": 6.284061758969376e-05,
      "loss": 4.9241,
      "step": 2100
    },
    {
      "epoch": 2.198212005108557,
      "grad_norm": 2.250791072845459,
      "learning_rate": 6.12493278627289e-05,
      "loss": 4.9487,
      "step": 2150
    },
    {
      "epoch": 2.2492975734355043,
      "grad_norm": 2.502894163131714,
      "learning_rate": 5.9645941729024124e-05,
      "loss": 4.8878,
      "step": 2200
    },
    {
      "epoch": 2.300383141762452,
      "grad_norm": 2.3319168090820312,
      "learning_rate": 5.803218331032883e-05,
      "loss": 4.9046,
      "step": 2250
    },
    {
      "epoch": 2.3514687100894,
      "grad_norm": 2.4540345668792725,
      "learning_rate": 5.640978788171467e-05,
      "loss": 4.8975,
      "step": 2300
    },
    {
      "epoch": 2.402554278416347,
      "grad_norm": 2.4277355670928955,
      "learning_rate": 5.478050000563341e-05,
      "loss": 4.9165,
      "step": 2350
    },
    {
      "epoch": 2.453639846743295,
      "grad_norm": 2.485288381576538,
      "learning_rate": 5.3146071655988417e-05,
      "loss": 4.875,
      "step": 2400
    },
    {
      "epoch": 2.5047254150702427,
      "grad_norm": 2.477464437484741,
      "learning_rate": 5.1508260334236247e-05,
      "loss": 4.8683,
      "step": 2450
    },
    {
      "epoch": 2.5558109833971905,
      "grad_norm": 2.878523349761963,
      "learning_rate": 4.986882717954481e-05,
      "loss": 4.8524,
      "step": 2500
    },
    {
      "epoch": 2.606896551724138,
      "grad_norm": 2.562422752380371,
      "learning_rate": 4.822953507503955e-05,
      "loss": 4.8676,
      "step": 2550
    },
    {
      "epoch": 2.6579821200510856,
      "grad_norm": 2.5370070934295654,
      "learning_rate": 4.659214675217461e-05,
      "loss": 4.8501,
      "step": 2600
    },
    {
      "epoch": 2.7090676883780334,
      "grad_norm": 2.752847194671631,
      "learning_rate": 4.4958422895267015e-05,
      "loss": 4.8872,
      "step": 2650
    },
    {
      "epoch": 2.7601532567049807,
      "grad_norm": 2.8296620845794678,
      "learning_rate": 4.3330120248231986e-05,
      "loss": 4.8541,
      "step": 2700
    },
    {
      "epoch": 2.8112388250319285,
      "grad_norm": 2.6731832027435303,
      "learning_rate": 4.170898972555571e-05,
      "loss": 4.8328,
      "step": 2750
    },
    {
      "epoch": 2.8623243933588762,
      "grad_norm": 2.7534029483795166,
      "learning_rate": 4.009677452953612e-05,
      "loss": 4.8525,
      "step": 2800
    },
    {
      "epoch": 2.913409961685824,
      "grad_norm": 2.5203607082366943,
      "learning_rate": 3.849520827581689e-05,
      "loss": 4.8631,
      "step": 2850
    },
    {
      "epoch": 2.9644955300127713,
      "grad_norm": 2.690312385559082,
      "learning_rate": 3.690601312922974e-05,
      "loss": 4.8297,
      "step": 2900
    },
    {
      "epoch": 2.9992337164750955,
      "eval_loss": 5.700453758239746,
      "eval_runtime": 3.7837,
      "eval_samples_per_second": 805.292,
      "eval_steps_per_second": 100.695,
      "step": 2934
    },
    {
      "epoch": 3.016347381864623,
      "grad_norm": 2.5958733558654785,
      "learning_rate": 3.533089795194997e-05,
      "loss": 4.9453,
      "step": 2950
    },
    {
      "epoch": 3.067432950191571,
      "grad_norm": 2.955784320831299,
      "learning_rate": 3.377155646595629e-05,
      "loss": 4.768,
      "step": 3000
    },
    {
      "epoch": 3.1185185185185187,
      "grad_norm": 3.082078218460083,
      "learning_rate": 3.222966543177085e-05,
      "loss": 4.7546,
      "step": 3050
    },
    {
      "epoch": 3.169604086845466,
      "grad_norm": 3.0007333755493164,
      "learning_rate": 3.0706882845438e-05,
      "loss": 4.7102,
      "step": 3100
    },
    {
      "epoch": 3.220689655172414,
      "grad_norm": 2.704265832901001,
      "learning_rate": 2.920484615568051e-05,
      "loss": 4.7478,
      "step": 3150
    },
    {
      "epoch": 3.2717752234993616,
      "grad_norm": 3.2221250534057617,
      "learning_rate": 2.7725170503150267e-05,
      "loss": 4.7535,
      "step": 3200
    },
    {
      "epoch": 3.322860791826309,
      "grad_norm": 2.774003267288208,
      "learning_rate": 2.6269446983666933e-05,
      "loss": 4.7587,
      "step": 3250
    },
    {
      "epoch": 3.3739463601532567,
      "grad_norm": 2.963083028793335,
      "learning_rate": 2.4839240937311918e-05,
      "loss": 4.7288,
      "step": 3300
    },
    {
      "epoch": 3.4250319284802044,
      "grad_norm": 3.3814468383789062,
      "learning_rate": 2.343609026521758e-05,
      "loss": 4.7377,
      "step": 3350
    },
    {
      "epoch": 3.476117496807152,
      "grad_norm": 3.329501152038574,
      "learning_rate": 2.206150377586157e-05,
      "loss": 4.7437,
      "step": 3400
    },
    {
      "epoch": 3.5272030651340995,
      "grad_norm": 3.0247714519500732,
      "learning_rate": 2.0716959562644457e-05,
      "loss": 4.7361,
      "step": 3450
    },
    {
      "epoch": 3.5782886334610473,
      "grad_norm": 3.070770502090454,
      "learning_rate": 1.940390341449522e-05,
      "loss": 4.7366,
      "step": 3500
    },
    {
      "epoch": 3.6293742017879946,
      "grad_norm": 3.1919033527374268,
      "learning_rate": 1.8123747261213996e-05,
      "loss": 4.6996,
      "step": 3550
    },
    {
      "epoch": 3.6804597701149424,
      "grad_norm": 3.186237096786499,
      "learning_rate": 1.6877867655223224e-05,
      "loss": 4.7362,
      "step": 3600
    },
    {
      "epoch": 3.73154533844189,
      "grad_norm": 3.151088237762451,
      "learning_rate": 1.5667604291360428e-05,
      "loss": 4.7077,
      "step": 3650
    },
    {
      "epoch": 3.782630906768838,
      "grad_norm": 3.241426944732666,
      "learning_rate": 1.4494258566303714e-05,
      "loss": 4.714,
      "step": 3700
    },
    {
      "epoch": 3.8337164750957853,
      "grad_norm": 3.5127174854278564,
      "learning_rate": 1.3359092179179478e-05,
      "loss": 4.7225,
      "step": 3750
    },
    {
      "epoch": 3.884802043422733,
      "grad_norm": 3.0208210945129395,
      "learning_rate": 1.2263325774856804e-05,
      "loss": 4.7416,
      "step": 3800
    },
    {
      "epoch": 3.935887611749681,
      "grad_norm": 3.3400168418884277,
      "learning_rate": 1.1208137631387517e-05,
      "loss": 4.6951,
      "step": 3850
    },
    {
      "epoch": 3.986973180076628,
      "grad_norm": 3.246314764022827,
      "learning_rate": 1.0194662393003367e-05,
      "loss": 4.7522,
      "step": 3900
    },
    {
      "epoch": 3.9992337164750955,
      "eval_loss": 5.691287040710449,
      "eval_runtime": 3.8281,
      "eval_samples_per_second": 795.959,
      "eval_steps_per_second": 99.528,
      "step": 3912
    }
  ],
  "logging_steps": 50,
  "max_steps": 4890,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 160565380681728.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

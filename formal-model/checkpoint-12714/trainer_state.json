{
  "best_global_step": 4890,
  "best_metric": 5.580859661102295,
  "best_model_checkpoint": "/Users/jitkamuravska/Neural-Networks/project/formal-model/checkpoint-4890",
  "epoch": 12.999233716475096,
  "eval_steps": 500,
  "global_step": 12714,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.05108556832694764,
      "grad_norm": 1.8662275075912476,
      "learning_rate": 4.9e-05,
      "loss": 8.9722,
      "step": 50
    },
    {
      "epoch": 0.10217113665389528,
      "grad_norm": 1.4961140155792236,
      "learning_rate": 9.900000000000001e-05,
      "loss": 8.4238,
      "step": 100
    },
    {
      "epoch": 0.1532567049808429,
      "grad_norm": 1.3672544956207275,
      "learning_rate": 9.999975123349196e-05,
      "loss": 7.6629,
      "step": 150
    },
    {
      "epoch": 0.20434227330779056,
      "grad_norm": 1.3869013786315918,
      "learning_rate": 9.999898452548349e-05,
      "loss": 7.0299,
      "step": 200
    },
    {
      "epoch": 0.2554278416347382,
      "grad_norm": 1.25546133518219,
      "learning_rate": 9.99976997803088e-05,
      "loss": 6.5652,
      "step": 250
    },
    {
      "epoch": 0.3065134099616858,
      "grad_norm": 1.0079456567764282,
      "learning_rate": 9.999589701127905e-05,
      "loss": 6.2051,
      "step": 300
    },
    {
      "epoch": 0.35759897828863346,
      "grad_norm": 0.7060031294822693,
      "learning_rate": 9.999357623707268e-05,
      "loss": 6.001,
      "step": 350
    },
    {
      "epoch": 0.4086845466155811,
      "grad_norm": 0.8253337740898132,
      "learning_rate": 9.999073748173511e-05,
      "loss": 5.911,
      "step": 400
    },
    {
      "epoch": 0.45977011494252873,
      "grad_norm": 0.8400639891624451,
      "learning_rate": 9.998738077467856e-05,
      "loss": 5.8145,
      "step": 450
    },
    {
      "epoch": 0.5108556832694764,
      "grad_norm": 0.9871360063552856,
      "learning_rate": 9.99835061506817e-05,
      "loss": 5.775,
      "step": 500
    },
    {
      "epoch": 0.561941251596424,
      "grad_norm": 0.9554833173751831,
      "learning_rate": 9.997911364988934e-05,
      "loss": 5.6916,
      "step": 550
    },
    {
      "epoch": 0.6130268199233716,
      "grad_norm": 1.047343134880066,
      "learning_rate": 9.9974203317812e-05,
      "loss": 5.6548,
      "step": 600
    },
    {
      "epoch": 0.6641123882503193,
      "grad_norm": 1.0431911945343018,
      "learning_rate": 9.996877520532535e-05,
      "loss": 5.6433,
      "step": 650
    },
    {
      "epoch": 0.7151979565772669,
      "grad_norm": 1.1472417116165161,
      "learning_rate": 9.996282936866987e-05,
      "loss": 5.5491,
      "step": 700
    },
    {
      "epoch": 0.7662835249042146,
      "grad_norm": 1.3801339864730835,
      "learning_rate": 9.995636586945006e-05,
      "loss": 5.5183,
      "step": 750
    },
    {
      "epoch": 0.8173690932311622,
      "grad_norm": 1.2019739151000977,
      "learning_rate": 9.994938477463395e-05,
      "loss": 5.4667,
      "step": 800
    },
    {
      "epoch": 0.8684546615581098,
      "grad_norm": 1.5513566732406616,
      "learning_rate": 9.994188615655234e-05,
      "loss": 5.437,
      "step": 850
    },
    {
      "epoch": 0.9195402298850575,
      "grad_norm": 1.5924468040466309,
      "learning_rate": 9.993387009289806e-05,
      "loss": 5.4289,
      "step": 900
    },
    {
      "epoch": 0.9706257982120051,
      "grad_norm": 1.541088342666626,
      "learning_rate": 9.99253366667252e-05,
      "loss": 5.3952,
      "step": 950
    },
    {
      "epoch": 0.9992337164750957,
      "eval_loss": 6.045016765594482,
      "eval_runtime": 3.8743,
      "eval_samples_per_second": 786.462,
      "eval_steps_per_second": 98.34,
      "step": 978
    },
    {
      "epoch": 1.022477650063857,
      "grad_norm": 1.4002498388290405,
      "learning_rate": 9.991628596644814e-05,
      "loss": 5.4277,
      "step": 1000
    },
    {
      "epoch": 1.0735632183908046,
      "grad_norm": 1.6753565073013306,
      "learning_rate": 9.990671808584082e-05,
      "loss": 5.2711,
      "step": 1050
    },
    {
      "epoch": 1.1246487867177521,
      "grad_norm": 1.740960717201233,
      "learning_rate": 9.989663312403559e-05,
      "loss": 5.2693,
      "step": 1100
    },
    {
      "epoch": 1.1757343550447,
      "grad_norm": 1.6927320957183838,
      "learning_rate": 9.988603118552226e-05,
      "loss": 5.2486,
      "step": 1150
    },
    {
      "epoch": 1.2268199233716475,
      "grad_norm": 1.6456506252288818,
      "learning_rate": 9.987491238014706e-05,
      "loss": 5.1847,
      "step": 1200
    },
    {
      "epoch": 1.2779054916985952,
      "grad_norm": 1.7662850618362427,
      "learning_rate": 9.986327682311137e-05,
      "loss": 5.1886,
      "step": 1250
    },
    {
      "epoch": 1.3289910600255428,
      "grad_norm": 1.765599250793457,
      "learning_rate": 9.985112463497068e-05,
      "loss": 5.1634,
      "step": 1300
    },
    {
      "epoch": 1.3800766283524903,
      "grad_norm": 2.007007360458374,
      "learning_rate": 9.983845594163329e-05,
      "loss": 5.164,
      "step": 1350
    },
    {
      "epoch": 1.4311621966794381,
      "grad_norm": 1.8880209922790527,
      "learning_rate": 9.982527087435887e-05,
      "loss": 5.1643,
      "step": 1400
    },
    {
      "epoch": 1.4822477650063857,
      "grad_norm": 2.093813896179199,
      "learning_rate": 9.981156956975734e-05,
      "loss": 5.1326,
      "step": 1450
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.9735324382781982,
      "learning_rate": 9.979735216978728e-05,
      "loss": 5.1406,
      "step": 1500
    },
    {
      "epoch": 1.584418901660281,
      "grad_norm": 1.8755534887313843,
      "learning_rate": 9.978261882175449e-05,
      "loss": 5.0821,
      "step": 1550
    },
    {
      "epoch": 1.6355044699872288,
      "grad_norm": 1.9053765535354614,
      "learning_rate": 9.976736967831054e-05,
      "loss": 5.0714,
      "step": 1600
    },
    {
      "epoch": 1.686590038314176,
      "grad_norm": 1.9680168628692627,
      "learning_rate": 9.975160489745107e-05,
      "loss": 5.0648,
      "step": 1650
    },
    {
      "epoch": 1.7376756066411239,
      "grad_norm": 1.9129551649093628,
      "learning_rate": 9.973532464251427e-05,
      "loss": 5.0573,
      "step": 1700
    },
    {
      "epoch": 1.7887611749680716,
      "grad_norm": 2.1873199939727783,
      "learning_rate": 9.97185290821791e-05,
      "loss": 5.0563,
      "step": 1750
    },
    {
      "epoch": 1.8398467432950192,
      "grad_norm": 1.9545478820800781,
      "learning_rate": 9.970121839046359e-05,
      "loss": 5.0185,
      "step": 1800
    },
    {
      "epoch": 1.8909323116219667,
      "grad_norm": 2.0174474716186523,
      "learning_rate": 9.9683392746723e-05,
      "loss": 5.0233,
      "step": 1850
    },
    {
      "epoch": 1.9420178799489145,
      "grad_norm": 2.07143497467041,
      "learning_rate": 9.9665052335648e-05,
      "loss": 5.0064,
      "step": 1900
    },
    {
      "epoch": 1.993103448275862,
      "grad_norm": 2.358407497406006,
      "learning_rate": 9.964619734726276e-05,
      "loss": 4.9633,
      "step": 1950
    },
    {
      "epoch": 1.9992337164750957,
      "eval_loss": 5.758966445922852,
      "eval_runtime": 3.8494,
      "eval_samples_per_second": 791.561,
      "eval_steps_per_second": 98.978,
      "step": 1956
    },
    {
      "epoch": 2.044955300127714,
      "grad_norm": 2.0689446926116943,
      "learning_rate": 9.962682797692293e-05,
      "loss": 5.0241,
      "step": 2000
    },
    {
      "epoch": 2.0960408684546614,
      "grad_norm": 2.091104745864868,
      "learning_rate": 9.960694442531366e-05,
      "loss": 4.8765,
      "step": 2050
    },
    {
      "epoch": 2.147126436781609,
      "grad_norm": 2.5554373264312744,
      "learning_rate": 9.958654689844751e-05,
      "loss": 4.8725,
      "step": 2100
    },
    {
      "epoch": 2.198212005108557,
      "grad_norm": 2.25459361076355,
      "learning_rate": 9.956563560766233e-05,
      "loss": 4.897,
      "step": 2150
    },
    {
      "epoch": 2.2492975734355043,
      "grad_norm": 2.514876365661621,
      "learning_rate": 9.9544210769619e-05,
      "loss": 4.8355,
      "step": 2200
    },
    {
      "epoch": 2.300383141762452,
      "grad_norm": 2.2464118003845215,
      "learning_rate": 9.95222726062993e-05,
      "loss": 4.8513,
      "step": 2250
    },
    {
      "epoch": 2.3514687100894,
      "grad_norm": 2.4446651935577393,
      "learning_rate": 9.949982134500352e-05,
      "loss": 4.841,
      "step": 2300
    },
    {
      "epoch": 2.402554278416347,
      "grad_norm": 2.353930950164795,
      "learning_rate": 9.947685721834814e-05,
      "loss": 4.8594,
      "step": 2350
    },
    {
      "epoch": 2.453639846743295,
      "grad_norm": 2.536482334136963,
      "learning_rate": 9.945338046426342e-05,
      "loss": 4.8168,
      "step": 2400
    },
    {
      "epoch": 2.5047254150702427,
      "grad_norm": 2.4075815677642822,
      "learning_rate": 9.942939132599093e-05,
      "loss": 4.8122,
      "step": 2450
    },
    {
      "epoch": 2.5558109833971905,
      "grad_norm": 2.6635890007019043,
      "learning_rate": 9.9404890052081e-05,
      "loss": 4.794,
      "step": 2500
    },
    {
      "epoch": 2.606896551724138,
      "grad_norm": 2.4591925144195557,
      "learning_rate": 9.937987689639022e-05,
      "loss": 4.8029,
      "step": 2550
    },
    {
      "epoch": 2.6579821200510856,
      "grad_norm": 2.4812896251678467,
      "learning_rate": 9.935435211807871e-05,
      "loss": 4.783,
      "step": 2600
    },
    {
      "epoch": 2.7090676883780334,
      "grad_norm": 2.634550094604492,
      "learning_rate": 9.932831598160753e-05,
      "loss": 4.8223,
      "step": 2650
    },
    {
      "epoch": 2.7601532567049807,
      "grad_norm": 2.7090260982513428,
      "learning_rate": 9.93017687567359e-05,
      "loss": 4.7874,
      "step": 2700
    },
    {
      "epoch": 2.8112388250319285,
      "grad_norm": 2.4697964191436768,
      "learning_rate": 9.927471071851838e-05,
      "loss": 4.7587,
      "step": 2750
    },
    {
      "epoch": 2.8623243933588762,
      "grad_norm": 2.643700361251831,
      "learning_rate": 9.924714214730205e-05,
      "loss": 4.7769,
      "step": 2800
    },
    {
      "epoch": 2.913409961685824,
      "grad_norm": 2.4894485473632812,
      "learning_rate": 9.92190633287236e-05,
      "loss": 4.7888,
      "step": 2850
    },
    {
      "epoch": 2.9644955300127713,
      "grad_norm": 2.5040481090545654,
      "learning_rate": 9.919047455370638e-05,
      "loss": 4.7518,
      "step": 2900
    },
    {
      "epoch": 2.9992337164750955,
      "eval_loss": 5.629762172698975,
      "eval_runtime": 3.8169,
      "eval_samples_per_second": 798.288,
      "eval_steps_per_second": 99.819,
      "step": 2934
    },
    {
      "epoch": 3.016347381864623,
      "grad_norm": 2.4975507259368896,
      "learning_rate": 9.916137611845735e-05,
      "loss": 4.8525,
      "step": 2950
    },
    {
      "epoch": 3.067432950191571,
      "grad_norm": 3.0392165184020996,
      "learning_rate": 9.913176832446407e-05,
      "loss": 4.6426,
      "step": 3000
    },
    {
      "epoch": 3.1185185185185187,
      "grad_norm": 2.998896598815918,
      "learning_rate": 9.910165147849151e-05,
      "loss": 4.6322,
      "step": 3050
    },
    {
      "epoch": 3.169604086845466,
      "grad_norm": 2.9961280822753906,
      "learning_rate": 9.907102589257893e-05,
      "loss": 4.5865,
      "step": 3100
    },
    {
      "epoch": 3.220689655172414,
      "grad_norm": 2.862717390060425,
      "learning_rate": 9.90398918840366e-05,
      "loss": 4.6222,
      "step": 3150
    },
    {
      "epoch": 3.2717752234993616,
      "grad_norm": 3.1879706382751465,
      "learning_rate": 9.900824977544254e-05,
      "loss": 4.6281,
      "step": 3200
    },
    {
      "epoch": 3.322860791826309,
      "grad_norm": 2.9453580379486084,
      "learning_rate": 9.897609989463917e-05,
      "loss": 4.6285,
      "step": 3250
    },
    {
      "epoch": 3.3739463601532567,
      "grad_norm": 3.213768720626831,
      "learning_rate": 9.894344257472993e-05,
      "loss": 4.6011,
      "step": 3300
    },
    {
      "epoch": 3.4250319284802044,
      "grad_norm": 3.2102086544036865,
      "learning_rate": 9.891027815407582e-05,
      "loss": 4.614,
      "step": 3350
    },
    {
      "epoch": 3.476117496807152,
      "grad_norm": 3.3243234157562256,
      "learning_rate": 9.887660697629183e-05,
      "loss": 4.6108,
      "step": 3400
    },
    {
      "epoch": 3.5272030651340995,
      "grad_norm": 2.980597496032715,
      "learning_rate": 9.88424293902435e-05,
      "loss": 4.6018,
      "step": 3450
    },
    {
      "epoch": 3.5782886334610473,
      "grad_norm": 3.220170736312866,
      "learning_rate": 9.88077457500432e-05,
      "loss": 4.6003,
      "step": 3500
    },
    {
      "epoch": 3.6293742017879946,
      "grad_norm": 3.1658031940460205,
      "learning_rate": 9.877255641504653e-05,
      "loss": 4.5657,
      "step": 3550
    },
    {
      "epoch": 3.6804597701149424,
      "grad_norm": 2.935415506362915,
      "learning_rate": 9.873686174984857e-05,
      "loss": 4.5971,
      "step": 3600
    },
    {
      "epoch": 3.73154533844189,
      "grad_norm": 3.6222922801971436,
      "learning_rate": 9.870066212428011e-05,
      "loss": 4.5667,
      "step": 3650
    },
    {
      "epoch": 3.782630906768838,
      "grad_norm": 3.3352551460266113,
      "learning_rate": 9.866395791340375e-05,
      "loss": 4.5709,
      "step": 3700
    },
    {
      "epoch": 3.8337164750957853,
      "grad_norm": 3.1833903789520264,
      "learning_rate": 9.862674949751016e-05,
      "loss": 4.5839,
      "step": 3750
    },
    {
      "epoch": 3.884802043422733,
      "grad_norm": 3.1444618701934814,
      "learning_rate": 9.858903726211401e-05,
      "loss": 4.5963,
      "step": 3800
    },
    {
      "epoch": 3.935887611749681,
      "grad_norm": 3.447800636291504,
      "learning_rate": 9.855082159795e-05,
      "loss": 4.545,
      "step": 3850
    },
    {
      "epoch": 3.986973180076628,
      "grad_norm": 3.0736355781555176,
      "learning_rate": 9.851210290096891e-05,
      "loss": 4.6065,
      "step": 3900
    },
    {
      "epoch": 3.9992337164750955,
      "eval_loss": 5.594105243682861,
      "eval_runtime": 3.8586,
      "eval_samples_per_second": 789.67,
      "eval_steps_per_second": 98.741,
      "step": 3912
    },
    {
      "epoch": 4.03882503192848,
      "grad_norm": 3.578451156616211,
      "learning_rate": 9.847288157233332e-05,
      "loss": 4.575,
      "step": 3950
    },
    {
      "epoch": 4.089910600255428,
      "grad_norm": 3.696542739868164,
      "learning_rate": 9.843315801841362e-05,
      "loss": 4.4672,
      "step": 4000
    },
    {
      "epoch": 4.140996168582375,
      "grad_norm": 3.9406793117523193,
      "learning_rate": 9.839293265078371e-05,
      "loss": 4.4126,
      "step": 4050
    },
    {
      "epoch": 4.192081736909323,
      "grad_norm": 3.874096632003784,
      "learning_rate": 9.835220588621679e-05,
      "loss": 4.4104,
      "step": 4100
    },
    {
      "epoch": 4.243167305236271,
      "grad_norm": 3.512890577316284,
      "learning_rate": 9.831097814668093e-05,
      "loss": 4.4547,
      "step": 4150
    },
    {
      "epoch": 4.294252873563218,
      "grad_norm": 3.54256272315979,
      "learning_rate": 9.82692498593349e-05,
      "loss": 4.44,
      "step": 4200
    },
    {
      "epoch": 4.345338441890166,
      "grad_norm": 4.116394996643066,
      "learning_rate": 9.82270214565235e-05,
      "loss": 4.3901,
      "step": 4250
    },
    {
      "epoch": 4.396424010217114,
      "grad_norm": 3.847480058670044,
      "learning_rate": 9.818429337577328e-05,
      "loss": 4.4222,
      "step": 4300
    },
    {
      "epoch": 4.447509578544062,
      "grad_norm": 3.357124090194702,
      "learning_rate": 9.814106605978792e-05,
      "loss": 4.403,
      "step": 4350
    },
    {
      "epoch": 4.498595146871009,
      "grad_norm": 3.694394826889038,
      "learning_rate": 9.809733995644362e-05,
      "loss": 4.41,
      "step": 4400
    },
    {
      "epoch": 4.549680715197956,
      "grad_norm": 4.257242202758789,
      "learning_rate": 9.805311551878454e-05,
      "loss": 4.4237,
      "step": 4450
    },
    {
      "epoch": 4.600766283524904,
      "grad_norm": 3.7394893169403076,
      "learning_rate": 9.8008393205018e-05,
      "loss": 4.4169,
      "step": 4500
    },
    {
      "epoch": 4.651851851851852,
      "grad_norm": 3.708277940750122,
      "learning_rate": 9.796317347850985e-05,
      "loss": 4.3927,
      "step": 4550
    },
    {
      "epoch": 4.7029374201788,
      "grad_norm": 4.2020745277404785,
      "learning_rate": 9.791745680777957e-05,
      "loss": 4.3589,
      "step": 4600
    },
    {
      "epoch": 4.7540229885057474,
      "grad_norm": 3.778567314147949,
      "learning_rate": 9.787124366649548e-05,
      "loss": 4.3816,
      "step": 4650
    },
    {
      "epoch": 4.805108556832694,
      "grad_norm": 3.952855348587036,
      "learning_rate": 9.78245345334698e-05,
      "loss": 4.3659,
      "step": 4700
    },
    {
      "epoch": 4.856194125159642,
      "grad_norm": 3.735539436340332,
      "learning_rate": 9.777732989265369e-05,
      "loss": 4.4069,
      "step": 4750
    },
    {
      "epoch": 4.90727969348659,
      "grad_norm": 3.956825017929077,
      "learning_rate": 9.772963023313227e-05,
      "loss": 4.4032,
      "step": 4800
    },
    {
      "epoch": 4.958365261813538,
      "grad_norm": 3.903430938720703,
      "learning_rate": 9.768143604911946e-05,
      "loss": 4.3879,
      "step": 4850
    },
    {
      "epoch": 4.9992337164750955,
      "eval_loss": 5.580859661102295,
      "eval_runtime": 3.8808,
      "eval_samples_per_second": 785.157,
      "eval_steps_per_second": 98.177,
      "step": 4890
    },
    {
      "epoch": 5.01021711366539,
      "grad_norm": 3.6729576587677,
      "learning_rate": 9.763274783995303e-05,
      "loss": 4.4684,
      "step": 4900
    },
    {
      "epoch": 5.061302681992337,
      "grad_norm": 3.9759905338287354,
      "learning_rate": 9.758356611008923e-05,
      "loss": 4.2464,
      "step": 4950
    },
    {
      "epoch": 5.112388250319285,
      "grad_norm": 4.1131157875061035,
      "learning_rate": 9.753389136909769e-05,
      "loss": 4.2318,
      "step": 5000
    },
    {
      "epoch": 5.163473818646232,
      "grad_norm": 4.4619035720825195,
      "learning_rate": 9.74837241316561e-05,
      "loss": 4.231,
      "step": 5050
    },
    {
      "epoch": 5.21455938697318,
      "grad_norm": 3.9794065952301025,
      "learning_rate": 9.743306491754492e-05,
      "loss": 4.2402,
      "step": 5100
    },
    {
      "epoch": 5.265644955300128,
      "grad_norm": 4.435154914855957,
      "learning_rate": 9.738191425164188e-05,
      "loss": 4.2514,
      "step": 5150
    },
    {
      "epoch": 5.316730523627076,
      "grad_norm": 4.673924446105957,
      "learning_rate": 9.733027266391668e-05,
      "loss": 4.2192,
      "step": 5200
    },
    {
      "epoch": 5.3678160919540225,
      "grad_norm": 4.839085578918457,
      "learning_rate": 9.727814068942544e-05,
      "loss": 4.2328,
      "step": 5250
    },
    {
      "epoch": 5.41890166028097,
      "grad_norm": 4.893242835998535,
      "learning_rate": 9.722551886830511e-05,
      "loss": 4.2436,
      "step": 5300
    },
    {
      "epoch": 5.469987228607918,
      "grad_norm": 4.698770523071289,
      "learning_rate": 9.717240774576796e-05,
      "loss": 4.2402,
      "step": 5350
    },
    {
      "epoch": 5.521072796934866,
      "grad_norm": 4.461274147033691,
      "learning_rate": 9.711880787209587e-05,
      "loss": 4.2441,
      "step": 5400
    },
    {
      "epoch": 5.572158365261814,
      "grad_norm": 4.81405782699585,
      "learning_rate": 9.706471980263466e-05,
      "loss": 4.2399,
      "step": 5450
    },
    {
      "epoch": 5.623243933588761,
      "grad_norm": 4.481505870819092,
      "learning_rate": 9.70101440977883e-05,
      "loss": 4.2364,
      "step": 5500
    },
    {
      "epoch": 5.674329501915709,
      "grad_norm": 4.737814903259277,
      "learning_rate": 9.695508132301315e-05,
      "loss": 4.2539,
      "step": 5550
    },
    {
      "epoch": 5.725415070242656,
      "grad_norm": 4.731093883514404,
      "learning_rate": 9.689953204881208e-05,
      "loss": 4.2523,
      "step": 5600
    },
    {
      "epoch": 5.776500638569604,
      "grad_norm": 4.904600143432617,
      "learning_rate": 9.684349685072855e-05,
      "loss": 4.2483,
      "step": 5650
    },
    {
      "epoch": 5.827586206896552,
      "grad_norm": 4.666957378387451,
      "learning_rate": 9.678697630934066e-05,
      "loss": 4.1988,
      "step": 5700
    },
    {
      "epoch": 5.878671775223499,
      "grad_norm": 4.460443496704102,
      "learning_rate": 9.672997101025511e-05,
      "loss": 4.198,
      "step": 5750
    },
    {
      "epoch": 5.929757343550447,
      "grad_norm": 4.425405502319336,
      "learning_rate": 9.667248154410118e-05,
      "loss": 4.2027,
      "step": 5800
    },
    {
      "epoch": 5.980842911877395,
      "grad_norm": 4.612942218780518,
      "learning_rate": 9.661450850652456e-05,
      "loss": 4.2545,
      "step": 5850
    },
    {
      "epoch": 5.9992337164750955,
      "eval_loss": 5.621065616607666,
      "eval_runtime": 3.965,
      "eval_samples_per_second": 768.483,
      "eval_steps_per_second": 96.092,
      "step": 5868
    },
    {
      "epoch": 6.032694763729246,
      "grad_norm": 4.444689750671387,
      "learning_rate": 9.65560524981812e-05,
      "loss": 4.2044,
      "step": 5900
    },
    {
      "epoch": 6.083780332056194,
      "grad_norm": 4.44782018661499,
      "learning_rate": 9.649711412473113e-05,
      "loss": 4.0706,
      "step": 5950
    },
    {
      "epoch": 6.134865900383142,
      "grad_norm": 5.221075534820557,
      "learning_rate": 9.64376939968321e-05,
      "loss": 4.0559,
      "step": 6000
    },
    {
      "epoch": 6.18595146871009,
      "grad_norm": 4.901515960693359,
      "learning_rate": 9.63777927301333e-05,
      "loss": 4.0743,
      "step": 6050
    },
    {
      "epoch": 6.237037037037037,
      "grad_norm": 4.723784923553467,
      "learning_rate": 9.631741094526898e-05,
      "loss": 4.0715,
      "step": 6100
    },
    {
      "epoch": 6.288122605363984,
      "grad_norm": 4.799471855163574,
      "learning_rate": 9.625654926785203e-05,
      "loss": 4.0547,
      "step": 6150
    },
    {
      "epoch": 6.339208173690932,
      "grad_norm": 4.4221014976501465,
      "learning_rate": 9.619520832846748e-05,
      "loss": 4.0909,
      "step": 6200
    },
    {
      "epoch": 6.39029374201788,
      "grad_norm": 5.145920276641846,
      "learning_rate": 9.613338876266596e-05,
      "loss": 4.049,
      "step": 6250
    },
    {
      "epoch": 6.441379310344828,
      "grad_norm": 4.772195339202881,
      "learning_rate": 9.607109121095713e-05,
      "loss": 4.0725,
      "step": 6300
    },
    {
      "epoch": 6.492464878671775,
      "grad_norm": 5.42764949798584,
      "learning_rate": 9.600831631880304e-05,
      "loss": 4.0664,
      "step": 6350
    },
    {
      "epoch": 6.543550446998723,
      "grad_norm": 5.058612823486328,
      "learning_rate": 9.594506473661145e-05,
      "loss": 4.0667,
      "step": 6400
    },
    {
      "epoch": 6.594636015325671,
      "grad_norm": 4.998598575592041,
      "learning_rate": 9.588133711972906e-05,
      "loss": 4.0604,
      "step": 6450
    },
    {
      "epoch": 6.645721583652618,
      "grad_norm": 4.680676460266113,
      "learning_rate": 9.581713412843478e-05,
      "loss": 4.0849,
      "step": 6500
    },
    {
      "epoch": 6.6968071519795656,
      "grad_norm": 5.47954797744751,
      "learning_rate": 9.575245642793284e-05,
      "loss": 4.1018,
      "step": 6550
    },
    {
      "epoch": 6.747892720306513,
      "grad_norm": 5.247927188873291,
      "learning_rate": 9.568730468834588e-05,
      "loss": 4.0526,
      "step": 6600
    },
    {
      "epoch": 6.798978288633461,
      "grad_norm": 5.386386394500732,
      "learning_rate": 9.562167958470809e-05,
      "loss": 4.0705,
      "step": 6650
    },
    {
      "epoch": 6.850063856960409,
      "grad_norm": 5.753106117248535,
      "learning_rate": 9.555558179695809e-05,
      "loss": 4.0868,
      "step": 6700
    },
    {
      "epoch": 6.901149425287357,
      "grad_norm": 5.39251708984375,
      "learning_rate": 9.548901200993205e-05,
      "loss": 4.0611,
      "step": 6750
    },
    {
      "epoch": 6.952234993614304,
      "grad_norm": 5.679132461547852,
      "learning_rate": 9.542197091335643e-05,
      "loss": 4.0464,
      "step": 6800
    },
    {
      "epoch": 6.9992337164750955,
      "eval_loss": 5.689183235168457,
      "eval_runtime": 4.1706,
      "eval_samples_per_second": 730.587,
      "eval_steps_per_second": 91.353,
      "step": 6846
    },
    {
      "epoch": 7.004086845466156,
      "grad_norm": 4.979214191436768,
      "learning_rate": 9.535445920184094e-05,
      "loss": 4.1332,
      "step": 6850
    },
    {
      "epoch": 7.055172413793104,
      "grad_norm": 5.590640068054199,
      "learning_rate": 9.528647757487127e-05,
      "loss": 3.8817,
      "step": 6900
    },
    {
      "epoch": 7.106257982120051,
      "grad_norm": 5.467238426208496,
      "learning_rate": 9.521802673680196e-05,
      "loss": 3.8782,
      "step": 6950
    },
    {
      "epoch": 7.157343550446999,
      "grad_norm": 5.525945663452148,
      "learning_rate": 9.514910739684896e-05,
      "loss": 3.8855,
      "step": 7000
    },
    {
      "epoch": 7.208429118773946,
      "grad_norm": 5.5395379066467285,
      "learning_rate": 9.507972026908234e-05,
      "loss": 3.8961,
      "step": 7050
    },
    {
      "epoch": 7.259514687100894,
      "grad_norm": 5.972360134124756,
      "learning_rate": 9.500986607241895e-05,
      "loss": 3.8845,
      "step": 7100
    },
    {
      "epoch": 7.3106002554278415,
      "grad_norm": 6.531222343444824,
      "learning_rate": 9.493954553061488e-05,
      "loss": 3.9139,
      "step": 7150
    },
    {
      "epoch": 7.361685823754789,
      "grad_norm": 6.006266117095947,
      "learning_rate": 9.486875937225801e-05,
      "loss": 3.8878,
      "step": 7200
    },
    {
      "epoch": 7.412771392081737,
      "grad_norm": 5.30122709274292,
      "learning_rate": 9.479750833076047e-05,
      "loss": 3.893,
      "step": 7250
    },
    {
      "epoch": 7.463856960408685,
      "grad_norm": 6.120121002197266,
      "learning_rate": 9.472579314435098e-05,
      "loss": 3.934,
      "step": 7300
    },
    {
      "epoch": 7.514942528735633,
      "grad_norm": 5.616155624389648,
      "learning_rate": 9.465361455606727e-05,
      "loss": 3.8999,
      "step": 7350
    },
    {
      "epoch": 7.5660280970625795,
      "grad_norm": 5.751104354858398,
      "learning_rate": 9.458097331374838e-05,
      "loss": 3.9085,
      "step": 7400
    },
    {
      "epoch": 7.617113665389527,
      "grad_norm": 5.969719886779785,
      "learning_rate": 9.450787017002685e-05,
      "loss": 3.9335,
      "step": 7450
    },
    {
      "epoch": 7.668199233716475,
      "grad_norm": 6.075752258300781,
      "learning_rate": 9.443430588232096e-05,
      "loss": 3.941,
      "step": 7500
    },
    {
      "epoch": 7.719284802043423,
      "grad_norm": 5.644134044647217,
      "learning_rate": 9.436028121282691e-05,
      "loss": 3.9178,
      "step": 7550
    },
    {
      "epoch": 7.770370370370371,
      "grad_norm": 6.592913627624512,
      "learning_rate": 9.428579692851086e-05,
      "loss": 3.9029,
      "step": 7600
    },
    {
      "epoch": 7.821455938697318,
      "grad_norm": 6.520242691040039,
      "learning_rate": 9.421085380110103e-05,
      "loss": 3.9126,
      "step": 7650
    },
    {
      "epoch": 7.872541507024266,
      "grad_norm": 5.8022685050964355,
      "learning_rate": 9.413545260707974e-05,
      "loss": 3.9492,
      "step": 7700
    },
    {
      "epoch": 7.923627075351213,
      "grad_norm": 6.065643310546875,
      "learning_rate": 9.405959412767523e-05,
      "loss": 3.9253,
      "step": 7750
    },
    {
      "epoch": 7.974712643678161,
      "grad_norm": 5.782585144042969,
      "learning_rate": 9.398327914885372e-05,
      "loss": 3.89,
      "step": 7800
    },
    {
      "epoch": 7.9992337164750955,
      "eval_loss": 5.7558088302612305,
      "eval_runtime": 3.9138,
      "eval_samples_per_second": 778.529,
      "eval_steps_per_second": 97.348,
      "step": 7824
    },
    {
      "epoch": 8.026564495530012,
      "grad_norm": 5.133793354034424,
      "learning_rate": 9.390650846131115e-05,
      "loss": 3.8951,
      "step": 7850
    },
    {
      "epoch": 8.07765006385696,
      "grad_norm": 6.027272701263428,
      "learning_rate": 9.382928286046508e-05,
      "loss": 3.7104,
      "step": 7900
    },
    {
      "epoch": 8.128735632183908,
      "grad_norm": 6.081758499145508,
      "learning_rate": 9.375160314644639e-05,
      "loss": 3.7441,
      "step": 7950
    },
    {
      "epoch": 8.179821200510856,
      "grad_norm": 6.251650333404541,
      "learning_rate": 9.367347012409102e-05,
      "loss": 3.732,
      "step": 8000
    },
    {
      "epoch": 8.230906768837803,
      "grad_norm": 6.589601993560791,
      "learning_rate": 9.359488460293158e-05,
      "loss": 3.7609,
      "step": 8050
    },
    {
      "epoch": 8.28199233716475,
      "grad_norm": 6.886096954345703,
      "learning_rate": 9.351584739718901e-05,
      "loss": 3.7312,
      "step": 8100
    },
    {
      "epoch": 8.333077905491699,
      "grad_norm": 6.731578826904297,
      "learning_rate": 9.343635932576418e-05,
      "loss": 3.7434,
      "step": 8150
    },
    {
      "epoch": 8.384163473818646,
      "grad_norm": 6.4593825340271,
      "learning_rate": 9.335642121222927e-05,
      "loss": 3.7442,
      "step": 8200
    },
    {
      "epoch": 8.435249042145594,
      "grad_norm": 7.048228740692139,
      "learning_rate": 9.327603388481942e-05,
      "loss": 3.7821,
      "step": 8250
    },
    {
      "epoch": 8.486334610472541,
      "grad_norm": 6.299498081207275,
      "learning_rate": 9.319519817642402e-05,
      "loss": 3.745,
      "step": 8300
    },
    {
      "epoch": 8.53742017879949,
      "grad_norm": 5.847239971160889,
      "learning_rate": 9.311391492457806e-05,
      "loss": 3.7219,
      "step": 8350
    },
    {
      "epoch": 8.588505747126437,
      "grad_norm": 6.418338775634766,
      "learning_rate": 9.303218497145357e-05,
      "loss": 3.753,
      "step": 8400
    },
    {
      "epoch": 8.639591315453384,
      "grad_norm": 6.684145450592041,
      "learning_rate": 9.295000916385084e-05,
      "loss": 3.7508,
      "step": 8450
    },
    {
      "epoch": 8.690676883780332,
      "grad_norm": 7.2884440422058105,
      "learning_rate": 9.286738835318958e-05,
      "loss": 3.736,
      "step": 8500
    },
    {
      "epoch": 8.74176245210728,
      "grad_norm": 6.0907301902771,
      "learning_rate": 9.278432339550021e-05,
      "loss": 3.7441,
      "step": 8550
    },
    {
      "epoch": 8.792848020434228,
      "grad_norm": 6.874346733093262,
      "learning_rate": 9.270081515141489e-05,
      "loss": 3.7893,
      "step": 8600
    },
    {
      "epoch": 8.843933588761175,
      "grad_norm": 6.2174201011657715,
      "learning_rate": 9.261686448615869e-05,
      "loss": 3.788,
      "step": 8650
    },
    {
      "epoch": 8.895019157088123,
      "grad_norm": 7.496128559112549,
      "learning_rate": 9.253247226954059e-05,
      "loss": 3.7718,
      "step": 8700
    },
    {
      "epoch": 8.94610472541507,
      "grad_norm": 6.789472579956055,
      "learning_rate": 9.244763937594443e-05,
      "loss": 3.7643,
      "step": 8750
    },
    {
      "epoch": 8.997190293742017,
      "grad_norm": 7.272472858428955,
      "learning_rate": 9.236236668431987e-05,
      "loss": 3.7753,
      "step": 8800
    },
    {
      "epoch": 8.999233716475096,
      "eval_loss": 5.860831260681152,
      "eval_runtime": 3.9572,
      "eval_samples_per_second": 769.979,
      "eval_steps_per_second": 96.279,
      "step": 8802
    },
    {
      "epoch": 9.04904214559387,
      "grad_norm": 6.055159568786621,
      "learning_rate": 9.227665507817336e-05,
      "loss": 3.6538,
      "step": 8850
    },
    {
      "epoch": 9.100127713920818,
      "grad_norm": 6.017949104309082,
      "learning_rate": 9.219050544555884e-05,
      "loss": 3.5514,
      "step": 8900
    },
    {
      "epoch": 9.151213282247765,
      "grad_norm": 7.508646011352539,
      "learning_rate": 9.210391867906874e-05,
      "loss": 3.5905,
      "step": 8950
    },
    {
      "epoch": 9.202298850574712,
      "grad_norm": 6.546025276184082,
      "learning_rate": 9.201689567582449e-05,
      "loss": 3.5993,
      "step": 9000
    },
    {
      "epoch": 9.25338441890166,
      "grad_norm": 6.557852745056152,
      "learning_rate": 9.192943733746742e-05,
      "loss": 3.5677,
      "step": 9050
    },
    {
      "epoch": 9.304469987228607,
      "grad_norm": 7.816704273223877,
      "learning_rate": 9.184154457014931e-05,
      "loss": 3.5796,
      "step": 9100
    },
    {
      "epoch": 9.355555555555556,
      "grad_norm": 6.9335432052612305,
      "learning_rate": 9.17532182845231e-05,
      "loss": 3.5932,
      "step": 9150
    },
    {
      "epoch": 9.406641123882503,
      "grad_norm": 7.807993412017822,
      "learning_rate": 9.16644593957333e-05,
      "loss": 3.6063,
      "step": 9200
    },
    {
      "epoch": 9.457726692209452,
      "grad_norm": 7.338268280029297,
      "learning_rate": 9.157526882340668e-05,
      "loss": 3.5872,
      "step": 9250
    },
    {
      "epoch": 9.508812260536398,
      "grad_norm": 7.263381481170654,
      "learning_rate": 9.148564749164263e-05,
      "loss": 3.5792,
      "step": 9300
    },
    {
      "epoch": 9.559897828863345,
      "grad_norm": 7.082736492156982,
      "learning_rate": 9.139559632900361e-05,
      "loss": 3.6127,
      "step": 9350
    },
    {
      "epoch": 9.610983397190294,
      "grad_norm": 6.607870578765869,
      "learning_rate": 9.130511626850557e-05,
      "loss": 3.5889,
      "step": 9400
    },
    {
      "epoch": 9.662068965517241,
      "grad_norm": 7.073760986328125,
      "learning_rate": 9.121420824760824e-05,
      "loss": 3.6316,
      "step": 9450
    },
    {
      "epoch": 9.71315453384419,
      "grad_norm": 6.843271255493164,
      "learning_rate": 9.11228732082054e-05,
      "loss": 3.6481,
      "step": 9500
    },
    {
      "epoch": 9.764240102171136,
      "grad_norm": 7.5276265144348145,
      "learning_rate": 9.103111209661517e-05,
      "loss": 3.5962,
      "step": 9550
    },
    {
      "epoch": 9.815325670498083,
      "grad_norm": 7.516010284423828,
      "learning_rate": 9.09389258635702e-05,
      "loss": 3.6245,
      "step": 9600
    },
    {
      "epoch": 9.866411238825032,
      "grad_norm": 6.954421520233154,
      "learning_rate": 9.084631546420775e-05,
      "loss": 3.6088,
      "step": 9650
    },
    {
      "epoch": 9.917496807151979,
      "grad_norm": 7.785287857055664,
      "learning_rate": 9.075328185805993e-05,
      "loss": 3.6165,
      "step": 9700
    },
    {
      "epoch": 9.968582375478928,
      "grad_norm": 8.209546089172363,
      "learning_rate": 9.065982600904361e-05,
      "loss": 3.5891,
      "step": 9750
    },
    {
      "epoch": 9.999233716475096,
      "eval_loss": 5.95532751083374,
      "eval_runtime": 3.8847,
      "eval_samples_per_second": 784.36,
      "eval_steps_per_second": 98.077,
      "step": 9780
    },
    {
      "epoch": 10.02043422733078,
      "grad_norm": 6.698991298675537,
      "learning_rate": 9.05659488854505e-05,
      "loss": 3.609,
      "step": 9800
    },
    {
      "epoch": 10.071519795657727,
      "grad_norm": 7.125720024108887,
      "learning_rate": 9.047165145993716e-05,
      "loss": 3.4055,
      "step": 9850
    },
    {
      "epoch": 10.122605363984674,
      "grad_norm": 7.695198059082031,
      "learning_rate": 9.037693470951487e-05,
      "loss": 3.4186,
      "step": 9900
    },
    {
      "epoch": 10.173690932311622,
      "grad_norm": 7.735890865325928,
      "learning_rate": 9.028179961553947e-05,
      "loss": 3.4057,
      "step": 9950
    },
    {
      "epoch": 10.22477650063857,
      "grad_norm": 7.2385687828063965,
      "learning_rate": 9.018624716370128e-05,
      "loss": 3.4045,
      "step": 10000
    },
    {
      "epoch": 10.275862068965518,
      "grad_norm": 6.860307693481445,
      "learning_rate": 9.009027834401483e-05,
      "loss": 3.4386,
      "step": 10050
    },
    {
      "epoch": 10.326947637292465,
      "grad_norm": 7.144959449768066,
      "learning_rate": 8.999389415080863e-05,
      "loss": 3.4323,
      "step": 10100
    },
    {
      "epoch": 10.378033205619413,
      "grad_norm": 8.137378692626953,
      "learning_rate": 8.989709558271484e-05,
      "loss": 3.4285,
      "step": 10150
    },
    {
      "epoch": 10.42911877394636,
      "grad_norm": 7.991856575012207,
      "learning_rate": 8.979988364265895e-05,
      "loss": 3.459,
      "step": 10200
    },
    {
      "epoch": 10.480204342273307,
      "grad_norm": 8.103691101074219,
      "learning_rate": 8.970225933784938e-05,
      "loss": 3.4343,
      "step": 10250
    },
    {
      "epoch": 10.531289910600256,
      "grad_norm": 8.215426445007324,
      "learning_rate": 8.960422367976701e-05,
      "loss": 3.4408,
      "step": 10300
    },
    {
      "epoch": 10.582375478927203,
      "grad_norm": 7.662731647491455,
      "learning_rate": 8.95057776841548e-05,
      "loss": 3.4749,
      "step": 10350
    },
    {
      "epoch": 10.633461047254151,
      "grad_norm": 8.00195026397705,
      "learning_rate": 8.940692237100715e-05,
      "loss": 3.4523,
      "step": 10400
    },
    {
      "epoch": 10.684546615581098,
      "grad_norm": 8.117059707641602,
      "learning_rate": 8.930765876455936e-05,
      "loss": 3.4687,
      "step": 10450
    },
    {
      "epoch": 10.735632183908045,
      "grad_norm": 8.26098346710205,
      "learning_rate": 8.920798789327709e-05,
      "loss": 3.4879,
      "step": 10500
    },
    {
      "epoch": 10.786717752234994,
      "grad_norm": 8.28404426574707,
      "learning_rate": 8.910791078984561e-05,
      "loss": 3.4589,
      "step": 10550
    },
    {
      "epoch": 10.83780332056194,
      "grad_norm": 8.165475845336914,
      "learning_rate": 8.900742849115918e-05,
      "loss": 3.4782,
      "step": 10600
    },
    {
      "epoch": 10.88888888888889,
      "grad_norm": 7.973421573638916,
      "learning_rate": 8.890654203831024e-05,
      "loss": 3.4744,
      "step": 10650
    },
    {
      "epoch": 10.939974457215836,
      "grad_norm": 7.7083740234375,
      "learning_rate": 8.880525247657865e-05,
      "loss": 3.4836,
      "step": 10700
    },
    {
      "epoch": 10.991060025542785,
      "grad_norm": 9.004109382629395,
      "learning_rate": 8.870356085542091e-05,
      "loss": 3.4634,
      "step": 10750
    },
    {
      "epoch": 10.999233716475096,
      "eval_loss": 6.084616184234619,
      "eval_runtime": 4.0054,
      "eval_samples_per_second": 760.725,
      "eval_steps_per_second": 95.122,
      "step": 10758
    },
    {
      "epoch": 11.042911877394635,
      "grad_norm": 8.052778244018555,
      "learning_rate": 8.860146822845917e-05,
      "loss": 3.3428,
      "step": 10800
    },
    {
      "epoch": 11.093997445721584,
      "grad_norm": 8.1892671585083,
      "learning_rate": 8.84989756534704e-05,
      "loss": 3.2431,
      "step": 10850
    },
    {
      "epoch": 11.14508301404853,
      "grad_norm": 7.48417329788208,
      "learning_rate": 8.839608419237547e-05,
      "loss": 3.268,
      "step": 10900
    },
    {
      "epoch": 11.19616858237548,
      "grad_norm": 8.192117691040039,
      "learning_rate": 8.829279491122804e-05,
      "loss": 3.2483,
      "step": 10950
    },
    {
      "epoch": 11.247254150702426,
      "grad_norm": 8.802881240844727,
      "learning_rate": 8.818910888020358e-05,
      "loss": 3.2999,
      "step": 11000
    },
    {
      "epoch": 11.298339719029375,
      "grad_norm": 8.200700759887695,
      "learning_rate": 8.808502717358822e-05,
      "loss": 3.2929,
      "step": 11050
    },
    {
      "epoch": 11.349425287356322,
      "grad_norm": 9.553539276123047,
      "learning_rate": 8.798055086976776e-05,
      "loss": 3.2585,
      "step": 11100
    },
    {
      "epoch": 11.400510855683269,
      "grad_norm": 9.538345336914062,
      "learning_rate": 8.787568105121633e-05,
      "loss": 3.2707,
      "step": 11150
    },
    {
      "epoch": 11.451596424010217,
      "grad_norm": 8.547089576721191,
      "learning_rate": 8.777041880448527e-05,
      "loss": 3.3175,
      "step": 11200
    },
    {
      "epoch": 11.502681992337164,
      "grad_norm": 8.480968475341797,
      "learning_rate": 8.766476522019185e-05,
      "loss": 3.3153,
      "step": 11250
    },
    {
      "epoch": 11.553767560664113,
      "grad_norm": 9.038893699645996,
      "learning_rate": 8.755872139300798e-05,
      "loss": 3.3348,
      "step": 11300
    },
    {
      "epoch": 11.60485312899106,
      "grad_norm": 9.206977844238281,
      "learning_rate": 8.745228842164883e-05,
      "loss": 3.2916,
      "step": 11350
    },
    {
      "epoch": 11.655938697318007,
      "grad_norm": 8.782820701599121,
      "learning_rate": 8.734546740886152e-05,
      "loss": 3.2728,
      "step": 11400
    },
    {
      "epoch": 11.707024265644955,
      "grad_norm": 9.049288749694824,
      "learning_rate": 8.723825946141358e-05,
      "loss": 3.3093,
      "step": 11450
    },
    {
      "epoch": 11.758109833971902,
      "grad_norm": 7.918444633483887,
      "learning_rate": 8.713066569008163e-05,
      "loss": 3.3257,
      "step": 11500
    },
    {
      "epoch": 11.809195402298851,
      "grad_norm": 8.838457107543945,
      "learning_rate": 8.702268720963977e-05,
      "loss": 3.3262,
      "step": 11550
    },
    {
      "epoch": 11.860280970625798,
      "grad_norm": 9.790703773498535,
      "learning_rate": 8.691432513884802e-05,
      "loss": 3.3392,
      "step": 11600
    },
    {
      "epoch": 11.911366538952747,
      "grad_norm": 8.031448364257812,
      "learning_rate": 8.680558060044078e-05,
      "loss": 3.3177,
      "step": 11650
    },
    {
      "epoch": 11.962452107279693,
      "grad_norm": 9.661835670471191,
      "learning_rate": 8.669645472111518e-05,
      "loss": 3.3606,
      "step": 11700
    },
    {
      "epoch": 11.999233716475096,
      "eval_loss": 6.2211127281188965,
      "eval_runtime": 3.9296,
      "eval_samples_per_second": 775.391,
      "eval_steps_per_second": 96.956,
      "step": 11736
    },
    {
      "epoch": 12.014303959131546,
      "grad_norm": 8.014726638793945,
      "learning_rate": 8.658694863151943e-05,
      "loss": 3.3212,
      "step": 11750
    },
    {
      "epoch": 12.065389527458493,
      "grad_norm": 8.774163246154785,
      "learning_rate": 8.647706346624104e-05,
      "loss": 3.1164,
      "step": 11800
    },
    {
      "epoch": 12.116475095785441,
      "grad_norm": 8.121471405029297,
      "learning_rate": 8.636680036379516e-05,
      "loss": 3.0893,
      "step": 11850
    },
    {
      "epoch": 12.167560664112388,
      "grad_norm": 8.593388557434082,
      "learning_rate": 8.625616046661265e-05,
      "loss": 3.1162,
      "step": 11900
    },
    {
      "epoch": 12.218646232439337,
      "grad_norm": 9.572190284729004,
      "learning_rate": 8.614514492102837e-05,
      "loss": 3.1255,
      "step": 11950
    },
    {
      "epoch": 12.269731800766284,
      "grad_norm": 9.000982284545898,
      "learning_rate": 8.603375487726928e-05,
      "loss": 3.1075,
      "step": 12000
    },
    {
      "epoch": 12.32081736909323,
      "grad_norm": 9.809287071228027,
      "learning_rate": 8.592199148944246e-05,
      "loss": 3.141,
      "step": 12050
    },
    {
      "epoch": 12.37190293742018,
      "grad_norm": 8.873744010925293,
      "learning_rate": 8.580985591552322e-05,
      "loss": 3.1332,
      "step": 12100
    },
    {
      "epoch": 12.422988505747126,
      "grad_norm": 9.80854320526123,
      "learning_rate": 8.569734931734303e-05,
      "loss": 3.1648,
      "step": 12150
    },
    {
      "epoch": 12.474074074074075,
      "grad_norm": 8.655972480773926,
      "learning_rate": 8.558447286057754e-05,
      "loss": 3.159,
      "step": 12200
    },
    {
      "epoch": 12.525159642401022,
      "grad_norm": 8.852747917175293,
      "learning_rate": 8.547122771473454e-05,
      "loss": 3.1256,
      "step": 12250
    },
    {
      "epoch": 12.576245210727969,
      "grad_norm": 9.157928466796875,
      "learning_rate": 8.535761505314172e-05,
      "loss": 3.1636,
      "step": 12300
    },
    {
      "epoch": 12.627330779054917,
      "grad_norm": 9.363717079162598,
      "learning_rate": 8.52436360529346e-05,
      "loss": 3.1405,
      "step": 12350
    },
    {
      "epoch": 12.678416347381864,
      "grad_norm": 8.781196594238281,
      "learning_rate": 8.512929189504437e-05,
      "loss": 3.178,
      "step": 12400
    },
    {
      "epoch": 12.729501915708813,
      "grad_norm": 9.062496185302734,
      "learning_rate": 8.501458376418553e-05,
      "loss": 3.1523,
      "step": 12450
    },
    {
      "epoch": 12.78058748403576,
      "grad_norm": 10.104266166687012,
      "learning_rate": 8.489951284884372e-05,
      "loss": 3.201,
      "step": 12500
    },
    {
      "epoch": 12.831673052362708,
      "grad_norm": 9.96197509765625,
      "learning_rate": 8.478408034126338e-05,
      "loss": 3.194,
      "step": 12550
    },
    {
      "epoch": 12.882758620689655,
      "grad_norm": 8.678979873657227,
      "learning_rate": 8.46682874374354e-05,
      "loss": 3.1781,
      "step": 12600
    },
    {
      "epoch": 12.933844189016602,
      "grad_norm": 11.766092300415039,
      "learning_rate": 8.455213533708466e-05,
      "loss": 3.1809,
      "step": 12650
    },
    {
      "epoch": 12.98492975734355,
      "grad_norm": 10.212690353393555,
      "learning_rate": 8.443562524365774e-05,
      "loss": 3.204,
      "step": 12700
    },
    {
      "epoch": 12.999233716475096,
      "eval_loss": 6.358895301818848,
      "eval_runtime": 4.0014,
      "eval_samples_per_second": 761.481,
      "eval_steps_per_second": 95.216,
      "step": 12714
    }
  ],
  "logging_steps": 50,
  "max_steps": 48900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 50,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 521732194621440.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}

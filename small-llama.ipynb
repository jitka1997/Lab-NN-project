{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e013210b",
      "metadata": {},
      "source": [
        "# Training llama models from scratch\n",
        "\n",
        "Import all needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "1ae3615b-35a8-4429-9b62-6bfa77e7677e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm \n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer)\n",
        "\n",
        "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    PreTrainedTokenizerFast, \n",
        "    set_seed, \n",
        "    Trainer, \n",
        "    TrainingArguments, \n",
        "    DataCollatorForLanguageModeling, \n",
        "    LlamaForCausalLM, \n",
        "    LlamaConfig)\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a828c42b-e9f3-4da6-a0e8-cd77dfc1b316",
      "metadata": {},
      "source": [
        "### Paths\n",
        "\n",
        "Set paths to training data, eval data, and model directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "44e701f0-1f65-4065-8457-7b82a59a4353",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_files = ['/Users/jitkamuravska/Neural-Networks/10/large_book.txt',]\n",
        "\n",
        "eval_files = ['/Users/jitkamuravska/Neural-Networks/10/small_book.txt',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "id": "54dc6a5c-caed-4894-b44f-93aa8cf873c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = '/Users/jitkamuravska/Neural-Networks/10/model/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf4e385",
      "metadata": {},
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edbb4723",
      "metadata": {},
      "source": [
        "Initialize with BPE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "id": "ab1208c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e4b9a",
      "metadata": {},
      "source": [
        "Normalizer that sets everything to normal unicode, lowercase, and strips white spaces and accents\n",
        "\n",
        "(explanations here: https://huggingface.co/docs/tokenizers/components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "80ca9a83",
      "metadata": {},
      "outputs": [],
      "source": [
        "normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "8077ff90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": 184,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalizer.normalize_str(\"Héllò hôw are ü?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "2b7f0ee2",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a2ea64",
      "metadata": {},
      "source": [
        "Pre-tokenization (division of text into tokens on which BPE can be performed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "5e72d03e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "id": "5ff60ef2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'s\", (3, 5)),\n",
              " ('Ġtest', (5, 10)),\n",
              " ('Ġpre', (10, 14)),\n",
              " ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)),\n",
              " ('!', (27, 28))]"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7334f243-68c2-4ffb-9035-8dd44a6b2df1",
      "metadata": {},
      "source": [
        "Set vocab size, add special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "0b71260a",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=16000,) #special_tokens=[\"<|endoftext|>\", \"<pad>\",]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "id": "7dad1f84",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.train(files = ['/Users/jitkamuravska/Neural-Networks/10/large_book.txt'], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "id": "a8e94458",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['let', 'Ġus', 'Ġmake', 'Ġa', 'Ġmodel']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let us make a model\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b82e68",
      "metadata": {},
      "source": [
        "By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don’t want the offsets to include these whitespaces, then this PostProcessor must be used:\n",
        "\n",
        "(https://huggingface.co/docs/tokenizers/api/post-processors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "id": "19f57718",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "b3146c96",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'token'"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "420c4f4f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=Sequence(normalizers=[NFD(), Lowercase(), Strip(strip_left=True, strip_right=True), StripAccents()]), pre_tokenizer=ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True), post_processor=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"!\":0, \"#\":1, \"$\":2, \"%\":3, \"&\":4, \"(\":5, \")\":6, \"*\":7, \",\":8, \"-\":9, \".\":10, \"/\":11, \"0\":12, \"1\":13, \"2\":14, \"3\":15, \"4\":16, \"5\":17, \"6\":18, \"7\":19, \"8\":20, \"9\":21, \":\":22, \";\":23, \"?\":24, \"[\":25, \"]\":26, \"_\":27, \"a\":28, \"b\":29, \"c\":30, \"d\":31, \"e\":32, \"f\":33, \"g\":34, \"h\":35, \"i\":36, \"j\":37, \"k\":38, \"l\":39, \"m\":40, \"n\":41, \"o\":42, \"p\":43, \"q\":44, \"r\":45, \"s\":46, \"t\":47, \"u\":48, \"v\":49, \"w\":50, \"x\":51, \"y\":52, \"z\":53, \"¢\":54, \"¦\":55, \"±\":56, \"´\":57, \"µ\":58, \"¶\":59, \"·\":60, \"¹\":61, \"º\":62, \"¼\":63, \"½\":64, \"¿\":65, \"Ã\":66, \"Å\":67, \"Î\":68, \"Ï\":69, \"â\":70, \"Ġ\":71, \"Ģ\":72, \"ģ\":73, \"Ĥ\":74, \"ĥ\":75, \"Ħ\":76, \"ħ\":77, \"ĵ\":78, \"Ķ\":79, \"ĺ\":80, \"Ļ\":81, \"ľ\":82, \"Ŀ\":83, \"ł\":84, \"Ġt\":85, \"he\":86, \"Ġa\":87, \"âĢ\":88, \"in\":89, \"Ġthe\":90, \"re\":91, \"Ġs\":92, \"Ġw\":93, \"ou\":94, \"Ġo\":95, \"Ġm\":96, \"ha\":97, \"is\":98, ...}, merges=[(\"Ġ\", \"t\"), (\"h\", \"e\"), (\"Ġ\", \"a\"), (\"â\", \"Ģ\"), (\"i\", \"n\"), (\"Ġt\", \"he\"), (\"r\", \"e\"), (\"Ġ\", \"s\"), (\"Ġ\", \"w\"), (\"o\", \"u\"), (\"Ġ\", \"o\"), (\"Ġ\", \"m\"), (\"h\", \"a\"), (\"i\", \"s\"), (\"n\", \"d\"), (\"e\", \"d\"), (\"e\", \"r\"), (\"Ġ\", \"c\"), (\"Ġ\", \"b\"), (\"e\", \"n\"), (\"o\", \"n\"), (\"i\", \"t\"), (\"Ġ\", \"f\"), (\"âĢ\", \"ľ\"), (\"âĢ\", \"Ŀ\"), (\"o\", \"r\"), (\"Ġ\", \"d\"), (\"Ġt\", \"o\"), (\"l\", \"l\"), (\"a\", \"n\"), (\"Ġo\", \"f\"), (\"e\", \"s\"), (\"in\", \"g\"), (\"Ġ\", \"p\"), (\"Ġ\", \"h\"), (\"a\", \"r\"), (\"y\", \"ou\"), (\"Ġa\", \"nd\"), (\"Ġ\", \"he\"), (\"Ġ\", \"you\"), (\"a\", \"s\"), (\"Ġ\", \"ha\"), (\"Ġ\", \"in\"), (\"n\", \"o\"), (\"a\", \"t\"), (\"Ġ\", \"i\"), (\"Ġ\", \"l\"), (\"o\", \"m\"), (\"ha\", \"t\"), (\"v\", \"e\"), (\"s\", \"e\"), (\"i\", \"d\"), (\"Ġ\", \"e\"), (\"Ġ\", \"re\"), (\"Ġ\", \"no\"), (\"Ġb\", \"e\"), (\"i\", \"c\"), (\"l\", \"e\"), (\"s\", \"t\"), (\"en\", \"t\"), (\"Ġ\", \"g\"), (\".\", \"âĢĿ\"), (\"Ġw\", \"h\"), (\"Ġs\", \"a\"), (\"i\", \"m\"), (\"Ġh\", \"is\"), (\"c\", \"e\"), (\"l\", \"y\"), (\"Ġt\", \"h\"), (\"t\", \"he\"), (\"u\", \"t\"), (\"Ġo\", \"n\"), (\"Ġt\", \"hat\"), (\"a\", \"l\"), (\"l\", \"d\"), (\"a\", \"d\"), (\"it\", \"h\"), (\"r\", \"o\"), (\"i\", \"ll\"), (\"i\", \"on\"), (\"Ġw\", \"as\"), (\"Ġ\", \"it\"), (\"g\", \"h\"), (\"i\", \"r\"), (\"Ġa\", \"s\"), (\"Ġw\", \"ith\"), (\"Ġno\", \"t\"), (\"Ġf\", \"or\"), (\"Ġ\", \"is\"), (\",\", \"âĢĿ\"), (\"r\", \"i\"), (\"Ġ\", \"âĢľ\"), (\"Ġ\", \"v\"), (\"Ġm\", \"e\"), (\"Ġd\", \"e\"), (\"?\", \"âĢĿ\"), (\"a\", \"nd\"), (\"Ġ\", \"u\"), (\"Ġa\", \"t\"), ...]))"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "id": "27cd369d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "id": "7d3401dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'lets test this tokenizer.'"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456297ab",
      "metadata": {},
      "source": [
        "Save it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "817d568a",
      "metadata": {},
      "outputs": [],
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        "    pad_token=\"<pad>\",\n",
        ")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "id": "08d4c599",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/jitkamuravska/Neural-Networks/10/model/tokenizer/tokenizer_config.json',\n",
              " '/Users/jitkamuravska/Neural-Networks/10/model/tokenizer/special_tokens_map.json',\n",
              " '/Users/jitkamuravska/Neural-Networks/10/model/tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wrapped_tokenizer.save_pretrained(model_path+'tokenizer/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf5ef16",
      "metadata": {},
      "source": [
        "### Training "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727dff5b",
      "metadata": {},
      "source": [
        "Load tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "id": "02c13c8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path+'tokenizer/')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdee9ff6-57d5-47d2-b03a-7fde61390414",
      "metadata": {},
      "source": [
        "Load data (now for training):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "bd6cf1cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset('text', data_files={'train': training_files, \n",
        "                                           'validation': eval_files})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daebd899",
      "metadata": {},
      "source": [
        "Creates batches (https://huggingface.co/docs/transformers/pad_truncation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "id": "ba29f94d",
      "metadata": {},
      "outputs": [],
      "source": [
        "context_length = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "b39eace4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 61684\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 14909\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True)\n",
        "    \n",
        "    input_batch = []\n",
        "    \n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize, \n",
        "                                      batched=True, \n",
        "                                      remove_columns=raw_datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "cad15b04-6990-40bc-9c4f-d79a97b29787",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 61684\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 14909\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbb7482",
      "metadata": {},
      "source": [
        "Initiate new Llama with config as wished:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "id": "5fe1190f-7357-47a3-9cc3-00468439c619",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16002"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "d589d379",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = LlamaConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    hidden_size=16,\n",
        "    num_hidden_layers=2,\n",
        "    intermediate_size=16,\n",
        "    num_attention_heads=2,\n",
        "    bos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "    eos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "    pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
        "    max_position_embeddings=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de1e5e3-bea9-4ca2-bbe6-7b414e379fe8",
      "metadata": {},
      "source": [
        "Set seed for weight initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "4c99f330-8a8c-4878-ac9c-9e576e488be6",
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a9ab15-5773-4a81-ba4a-308f389093a1",
      "metadata": {},
      "source": [
        "New model object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "5a07e805",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LlamaForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "aa89da5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6343c2e-30bc-4e99-ab8d-9a2d3d4c37ae",
      "metadata": {},
      "source": [
        "Check out param size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "89f276d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model num parameters = 515728\n"
          ]
        }
      ],
      "source": [
        "print(f'model num parameters = {model.num_parameters()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "f2fb20ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_dict = config.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b112127-7368-4fc6-a6a5-e990bbbe625c",
      "metadata": {},
      "source": [
        "Set training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "22bee44e",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    overwrite_output_dir=True,\n",
        "    save_strategy = \"epoch\", # saves after every epoch\n",
        "    #save_strategy = \"steps\", \n",
        "    #save_steps = 0.1, # if below zero, then saves after every (n*100)% of training steps\n",
        "    save_total_limit=0,  # set to zero to avoid saving\n",
        "    eval_strategy = \"epoch\",\n",
        "    #eval_steps = 0.1,\n",
        "    num_train_epochs= 10,\n",
        "    #max_steps = 1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    per_device_train_batch_size=16,\n",
        "    warmup_steps=200, \n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=3e-4, # normal: 5e-4\n",
        "    logging_steps=10,\n",
        "    #fp16=True, ## only on CUDA\n",
        "    #load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    #use_mps_device=True, ## only on apple silicon\n",
        "    #use_cpu = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3537432-881c-42ed-83d4-fef5456efe45",
      "metadata": {},
      "source": [
        "Initialize trainer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "893b3d99",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ln/fhk1_2pd10n5h6m_z3xbszwm0000gn/T/ipykernel_17891/669889199.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets['train'],#[:15000]['input_ids'],\n",
        "    eval_dataset=tokenized_datasets['validation']#[:1200]['input_ids'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d20de00-46b5-478c-a92a-58f0e5433bd8",
      "metadata": {},
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "eca36080",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4820' max='4820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4820/4820 11:07, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>6.804700</td>\n",
              "      <td>7.633952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.930600</td>\n",
              "      <td>7.006900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.717200</td>\n",
              "      <td>6.785738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>5.453700</td>\n",
              "      <td>6.585369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>5.241900</td>\n",
              "      <td>6.425212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.105000</td>\n",
              "      <td>6.331116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>5.039300</td>\n",
              "      <td>6.280762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>4.944000</td>\n",
              "      <td>6.258432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>5.002300</td>\n",
              "      <td>6.250455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.904700</td>\n",
              "      <td>6.249041</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4820, training_loss=5.605137153878746, metrics={'train_runtime': 667.6729, 'train_samples_per_second': 923.866, 'train_steps_per_second': 7.219, 'total_flos': 32954269349760.0, 'train_loss': 5.605137153878746, 'epoch': 10.0})"
            ]
          },
          "execution_count": 212,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f7370f-bb06-44a8-b873-5519f5c31cb4",
      "metadata": {},
      "source": [
        "Save logs of losses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "6b00de27-3260-49fd-8865-6980e8bbe5c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)\n",
        "df.to_csv(model_path+'logs/losses.csv')  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336a05f8-34d0-402b-a785-0a729d6ebea0",
      "metadata": {},
      "source": [
        "Save final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "f68875c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(model_path+'final/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1debbe5e-1a7b-49bf-bde3-cd6c23e06608",
      "metadata": {},
      "source": [
        "### Test trained model on text generation\n",
        "\n",
        "With hf pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "a6a24ae6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model_path+'final/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "77720de3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'From the prison he remembers ,, who were not be in an hand, for i am is the letter,, “i am in m; i had just. you have to have been you,” said it to be as he am.” one.” monte cristo into the other. which i to his hands to i had to say that i were, i think of the door, it was as i do be in the young one of the one as who not have been, she replied i do one, i had?” said my young the young this count, “what shall know?” said at me, “i will you?” said is'},\n",
              " {'generated_text': 'From the prison he remembers , to be and your father?” said in the young two’s hand, and she is on the house to the father, like the letter of the count, for to be just. i were at them; if not not to not know all at me?” said a man of the count, “i will be the carriage, “you was a young other.” he is a good at the old man, he’s in the count,” said i should be in to the old man to my same young count by his room and the most so he not be a abbe, the time a small of that'},\n",
              " {'generated_text': 'From the prison he remembers  in an time. and the name is it he had not me, this, of the baroness of a very long,” said albert?” asked you, and all the head.” then that he may know her father, the own of its father, you to villefort of a first—what i have just they am, then and do to you have taken their heart. that the father, sir, “the abbe had not that he had be.” his be in such the door, and m. albert is not see my old of his head.” i have to one of his be a the child?” replied which you'},\n",
              " {'generated_text': 'From the prison he remembers  to do the word, for the end, and had been not not to the count’s the own be by the heart, of him to be the moment!” he has seen that the count’clock!” said and she!” exclaimed the most about?” asked a door to me a same.” the few, and you to a man of the man, when “and and m to it of the count, we have?” replied me from the count with me to be his house, “it is be very not say from two, and so my hand you am, and the sames man; to their old'},\n",
              " {'generated_text': 'From the prison he remembers  so who, and have not, and not had you will be very youngi have not, your own his be only to me, and so, in them, the a other. at you are with these words,” said, and to have not the young not that i shall, and he will be, de villefort. my young the same own eyes, “i the second, which i had already have been been of his friend, he had do more a man, they have said the very not that an day, as me to take her eyes of him and the count, and that you, with not'},\n",
              " {'generated_text': 'From the prison he remembers  with the door. “he shall not have.” i have the door. the old man of the other the the man, he would be in his count of our same one of madame morrel.” it you as you will not his. but you have had been and you. i am it is that was?” said,” continued albert of his son that she on the young man, as at your father.” the door, my door’s a young not be to see.” the baroness the count, he did for these.” me to the man for their old the own?” so.” if i, it, i'},\n",
              " {'generated_text': 'From the prison he remembers  is not him at the young with the carriage, for the poor name, and not?” said it which i do i am not to your dearyou have to me—and to what to the head.” the door or a count’s a first, and which i, the one—i can and your father and your house, your a father and the countess, “ man said, and as “oh, which, he may know my the son, the heart?” had not been it are it at the good. a same count,”,” said as they have the whole time. but as for you can'},\n",
              " {'generated_text': 'From the prison he remembers . but that man is not have to any time of his me, and and a man,” added i will tell a other?” said they am so that i is that the young this one and what, “i were so, he are you are you have to have been not the house, and you. i should your most it as i shall be to an-room. i am, “well, in the most-; there on at the man’s house; “no,000, you have been i were made i should not you may be a mother had not say i will be that he'},\n",
              " {'generated_text': 'From the prison he remembers  of the few name,, a not not that so. his me, at “i have me, thes man could have all the father, and not be his man by the house to be his excellency by the name, as if the the few, but a last house of the count. my the count of no a few o’ of dantes was your words to it is that if not not be?” replied monte cristo has not. you was my father’s man that he are you have said—well which he, for a count to know as you. you i have an house, on i'},\n",
              " {'generated_text': 'From the prison he remembers  and not a man. it a man had all a man of the count, and we would the man. monte in the father, m; and your mother, if i have you to that is all the young man. the dear “what to you are in your eyes it was you have at a son, he had had to the father, and he was to be as monte cristo was as monte cristo’s his eyes as a the man. monte cristo as with,, but not to some door, which, the old time and with their mind, to not had do to her was a abbe.'},\n",
              " {'generated_text': 'From the prison he remembers  to the count; and in his other, and you know not no house, “what as your be in the moment—you be, and is to a letter is not the door. the count is not?” asked you, he would be a young her be,” said from which my letter of the first, “i am been i. who was all a abbe’s me; ‘we’s, and i know him!” said who?” said a man was not are been and him. he is it would have a the door. he is his, you for my his first.” “ man?”'},\n",
              " {'generated_text': 'From the prison he remembers  of the young other; she had no man. i do that your hand to the count of the very son.”, but the most it of the father, his heart.” my two, and the house of the not have more and that all the count who had is be my life; his the father was been not for the man, and that?” said the room, what was a count when the poor room-room, and we am you have been he as who!” you. my be you not no. but and all.” franz had very little only to see we do. “it is you be'},\n",
              " {'generated_text': 'From the prison he remembers , “ah,” replied that you, “yes.” monte cristo, “you have all on the other. the other eyes by the own first young, if they was to a man, i only i tell that the a count by a friend was a door,” said this man,” is to be you the room, “he am not be it had not you had the countess is you were with himself to make the the count than you’s, and be a same, and me,” was at it of his eyes, i am a man, but “you on, and i will be this to be'},\n",
              " {'generated_text': 'From the prison he remembers  to his other, “is for the father was been not all the young in your, and he; i be a small as i have ans be one. i had been been a a son, but if my mother.” as i are to say a man was i do i will be at the other, as all the most young your, the most in our name that to do so in the young you are to a a two friend—that has have very long, he could said. i have already; to the not give not be so in the baroness, as we shall have been of all.”'},\n",
              " {'generated_text': 'From the prison he remembers , and the count was been the head of a dearyou know with the count, he was you by i have i was a door. he was the man.” the first. in his. nom of your abbe would see we i had been.” but the dear that is to your first; but a world,” had that my time, “yes of the a a man and the man cristo’s hand would see you?” said my the first. he was, my mind to the youngand you on one, and his. monte cristo have now, and i will be you, you with not the'},\n",
              " {'generated_text': 'From the prison he remembers  to, to it in no-in a young he could been been his name, and the count, there will i have been with the young the carriage’s own way, but you have a house. albert; danglars’s?” said by the room with himself at the word; it was on his father.” “and to he were very own other, with a count who may not is you have been. but i know in villefort was very friend?” said and one with the the two words of an father, who is to this not, and that, “that of their the not you and a'},\n",
              " {'generated_text': 'From the prison he remembers  his count, as on you with a hand of all the man’s?” said the count of the dearthat the world, the room’s day on all monte cristo of a young man, who are me, as they, and a man.” what that the count to the door of the door; what you have left with thes other’s door,” said the day, who and a letter, her man from the world, but the name in my eyes. i will, and you and have with his dear your father will, then at her to be for that the father’s your hand'},\n",
              " {'generated_text': 'From the prison he remembers ; “what say in the count, if, as my letter.” i might know the count, for i?” cried this, “you-bye, “oh this a room, but my own of “well, sir was all not be not had been with his eyes, and it is for him you should know; it is it to your same- been it to see we shall have only as for the dear his own, to see this first of m, “well, and have as on his oldi know i, they have was be be taken me; a name? you, by the young'},\n",
              " {'generated_text': 'From the prison he remembers , and do the man, the count of a man would have is so. the baroness was that is no-renaud’s the young thes. his be not a man’s twoly, “so said to the moment of the most in an, he was a most very very father.” it to a not; “oh, but i will make an house of the count?” said. de villefort,” said at the eyes, but this friend, and you shall have to not not, who at the count into my child of the good be a own in the father for a young man was my'},\n",
              " {'generated_text': 'From the prison he remembers . as he is you am not had it was a most the carriage was not take his own dear his word with it,” said monte cristo; i must be there would see to the room of madame, but you should. the a room of he was not be all by a moment, and his other, in the count; the other, and it has, “and so said, he was, or three-meran. with the house and have been of your own father. we have to the eyes, then, ‘for you that this of the young my young man had a count, and the first'}]"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe(\"From the prison he remembers \", do_sample = True, \n",
        "     num_return_sequences = 20, \n",
        "     max_length=128\n",
        "     #top_k=50,\n",
        "     #top_p=0.8,\n",
        "     #temperature=1.0,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91eb39a-4197-4aa7-a775-5596a366361c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

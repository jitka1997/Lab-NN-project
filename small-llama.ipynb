{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e013210b",
      "metadata": {},
      "source": [
        "# Training llama models from scratch\n",
        "\n",
        "Import all needed libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "1ae3615b-35a8-4429-9b62-6bfa77e7677e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from random import sample\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm \n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer)\n",
        "\n",
        "from tokenizers.normalizers import Lowercase, Strip, StripAccents, NFD\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    PreTrainedTokenizerFast, \n",
        "    set_seed, \n",
        "    Trainer, \n",
        "    TrainingArguments, \n",
        "    DataCollatorForLanguageModeling, \n",
        "    LlamaForCausalLM, \n",
        "    LlamaConfig)\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a828c42b-e9f3-4da6-a0e8-cd77dfc1b316",
      "metadata": {},
      "source": [
        "### Paths\n",
        "\n",
        "Set paths to training data, eval data, and model directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "44e701f0-1f65-4065-8457-7b82a59a4353",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_files = ['/Users/jitkamuravska/Neural-Networks/project/training_data/formal_train.txt',]\n",
        "\n",
        "eval_files = ['/Users/jitkamuravska/Neural-Networks/project/training_data/formal_val.txt',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "54dc6a5c-caed-4894-b44f-93aa8cf873c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = '/Users/jitkamuravska/Neural-Networks/project/formal-model/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbf4e385",
      "metadata": {},
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edbb4723",
      "metadata": {},
      "source": [
        "Initialize with BPE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "ab1208c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e4b9a",
      "metadata": {},
      "source": [
        "Normalizer that sets everything to normal unicode, lowercase, and strips white spaces and accents\n",
        "\n",
        "(explanations here: https://huggingface.co/docs/tokenizers/components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "80ca9a83",
      "metadata": {},
      "outputs": [],
      "source": [
        "normalizer = normalizers.Sequence([NFD(), Lowercase(), Strip(), StripAccents()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "8077ff90",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hello how are u?'"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalizer.normalize_str(\"Héllò hôw are ü?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "2b7f0ee2",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.normalizer = normalizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a2ea64",
      "metadata": {},
      "source": [
        "Pre-tokenization (division of text into tokens on which BPE can be performed):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "5e72d03e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "5ff60ef2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Let', (0, 3)),\n",
              " (\"'s\", (3, 5)),\n",
              " ('Ġtest', (5, 10)),\n",
              " ('Ġpre', (10, 14)),\n",
              " ('-', (14, 15)),\n",
              " ('tokenization', (15, 27)),\n",
              " ('!', (27, 28))]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7334f243-68c2-4ffb-9035-8dd44a6b2df1",
      "metadata": {},
      "source": [
        "Set vocab size, add special tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "0b71260a",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = trainers.BpeTrainer(vocab_size=16000,) #special_tokens=[\"<|endoftext|>\", \"<pad>\",]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "7dad1f84",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "tokenizer.train(files = ['/Users/jitkamuravska/Neural-Networks/project/training_data/formal_train.txt'], trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "a8e94458",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['let', 'Ġus', 'Ġmake', 'Ġa', 'Ġmodel']\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"Let us make a model\")\n",
        "print(encoding.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b82e68",
      "metadata": {},
      "source": [
        "By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don’t want the offsets to include these whitespaces, then this PostProcessor must be used:\n",
        "\n",
        "(https://huggingface.co/docs/tokenizers/api/post-processors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "19f57718",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "b3146c96",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'to'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = \"Let's test this tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "420c4f4f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=Sequence(normalizers=[NFD(), Lowercase(), Strip(strip_left=True, strip_right=True), StripAccents()]), pre_tokenizer=ByteLevel(add_prefix_space=False, trim_offsets=True, use_regex=True), post_processor=ByteLevel(add_prefix_space=True, trim_offsets=True, use_regex=True), decoder=None, model=BPE(dropout=None, unk_token=None, continuing_subword_prefix=None, end_of_word_suffix=None, fuse_unk=False, byte_fallback=False, ignore_merges=False, vocab={\"\"\":0, \"#\":1, \"$\":2, \"%\":3, \"&\":4, \"'\":5, \"(\":6, \")\":7, \"*\":8, \"+\":9, \",\":10, \"-\":11, \".\":12, \"/\":13, \"0\":14, \"1\":15, \"2\":16, \"3\":17, \"4\":18, \"5\":19, \"6\":20, \"7\":21, \"8\":22, \"9\":23, \":\":24, \";\":25, \"<\":26, \"=\":27, \">\":28, \"@\":29, \"[\":30, \"]\":31, \"_\":32, \"a\":33, \"b\":34, \"c\":35, \"d\":36, \"e\":37, \"f\":38, \"g\":39, \"h\":40, \"i\":41, \"j\":42, \"k\":43, \"l\":44, \"m\":45, \"n\":46, \"o\":47, \"p\":48, \"q\":49, \"r\":50, \"s\":51, \"t\":52, \"u\":53, \"v\":54, \"w\":55, \"x\":56, \"y\":57, \"z\":58, \"{\":59, \"}\":60, \"~\":61, \"¦\":62, \"°\":63, \"Â\":64, \"Ã\":65, \"â\":66, \"Ġ\":67, \"Ģ\":68, \"Ķ\":69, \"ĺ\":70, \"Ļ\":71, \"ľ\":72, \"Ŀ\":73, \"Ġt\":74, \"Ġth\":75, \"Ġa\":76, \"Ġthe\":77, \"Ġo\":78, \"in\":79, \"er\":80, \"on\":81, \"en\":82, \"re\":83, \"at\":84, \"Ġs\":85, \"Ġof\":86, \"it\":87, \"Ġb\":88, \"Ġw\":89, \"es\":90, \"Ġc\":91, \"ed\":92, \"is\":93, \"Ġp\":94, \"an\":95, \"Ġin\":96, \"Ġan\":97, \"Ġf\":98, ...}, merges=[(\"Ġ\", \"t\"), (\"Ġt\", \"h\"), (\"Ġ\", \"a\"), (\"Ġth\", \"e\"), (\"Ġ\", \"o\"), (\"i\", \"n\"), (\"e\", \"r\"), (\"o\", \"n\"), (\"e\", \"n\"), (\"r\", \"e\"), (\"a\", \"t\"), (\"Ġ\", \"s\"), (\"Ġo\", \"f\"), (\"i\", \"t\"), (\"Ġ\", \"b\"), (\"Ġ\", \"w\"), (\"e\", \"s\"), (\"Ġ\", \"c\"), (\"e\", \"d\"), (\"i\", \"s\"), (\"Ġ\", \"p\"), (\"a\", \"n\"), (\"Ġ\", \"in\"), (\"Ġa\", \"n\"), (\"Ġ\", \"f\"), (\"i\", \"on\"), (\"Ġ\", \"m\"), (\"o\", \"r\"), (\"a\", \"r\"), (\"a\", \"l\"), (\"Ġt\", \"o\"), (\"o\", \"u\"), (\"i\", \"c\"), (\"Ġ\", \"h\"), (\"Ġb\", \"e\"), (\"Ġan\", \"d\"), (\"Ġ\", \"d\"), (\"Ġ\", \"n\"), (\"en\", \"t\"), (\"t\", \"h\"), (\"l\", \"e\"), (\"Ġ\", \"e\"), (\"c\", \"t\"), (\"s\", \"t\"), (\"r\", \"o\"), (\"in\", \"g\"), (\"l\", \"y\"), (\"s\", \"e\"), (\"v\", \"e\"), (\"Ġw\", \"h\"), (\"l\", \"l\"), (\"Ġh\", \"a\"), (\"Ġ\", \"g\"), (\"Ġ\", \"l\"), (\"Ġ\", \"re\"), (\"v\", \"er\"), (\"c\", \"e\"), (\"at\", \"ion\"), (\"Ġ\", \"it\"), (\"o\", \"m\"), (\"u\", \"t\"), (\"Ġth\", \"at\"), (\"Ġc\", \"on\"), (\"i\", \"m\"), (\"Ġo\", \"n\"), (\"i\", \"f\"), (\"Ġf\", \"or\"), (\"Ġ\", \"u\"), (\"i\", \"r\"), (\"Ġ\", \"is\"), (\"p\", \"e\"), (\"u\", \"r\"), (\"Ġa\", \"s\"), (\"i\", \"es\"), (\"Ġs\", \"t\"), (\"l\", \"d\"), (\"Ġn\", \"o\"), (\"a\", \"s\"), (\"ic\", \"h\"), (\"i\", \"d\"), (\"Ġo\", \"r\"), (\"c\", \"h\"), (\"o\", \"w\"), (\"it\", \"h\"), (\"Ġe\", \"x\"), (\"i\", \"g\"), (\"Ġwh\", \"ich\"), (\"Ġb\", \"y\"), (\"at\", \"e\"), (\"Ġs\", \"e\"), (\"Ġha\", \"ve\"), (\"Ġs\", \"u\"), (\"ou\", \"ld\"), (\"th\", \"er\"), (\"a\", \"b\"), (\"Ġp\", \"ro\"), (\"i\", \"ll\"), (\"u\", \"l\"), (\"m\", \"ent\"), ...]))"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "27cd369d",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.decoder = decoders.ByteLevel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "7d3401dc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"let's test this tokenizer.\""
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456297ab",
      "metadata": {},
      "source": [
        "Save it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "817d568a",
      "metadata": {},
      "outputs": [],
      "source": [
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",\n",
        "    pad_token=\"<pad>\",\n",
        ")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "08d4c599",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/Users/jitkamuravska/Neural-Networks/project/formal-model/tokenizer/tokenizer_config.json',\n",
              " '/Users/jitkamuravska/Neural-Networks/project/formal-model/tokenizer/special_tokens_map.json',\n",
              " '/Users/jitkamuravska/Neural-Networks/project/formal-model/tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wrapped_tokenizer.save_pretrained(model_path+'tokenizer/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf5ef16",
      "metadata": {},
      "source": [
        "### Training "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727dff5b",
      "metadata": {},
      "source": [
        "Load tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "02c13c8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_path+'tokenizer/')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdee9ff6-57d5-47d2-b03a-7fde61390414",
      "metadata": {},
      "source": [
        "Load data (now for training):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "bd6cf1cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_datasets = load_dataset('text', data_files={'train': training_files, \n",
        "                                           'validation': eval_files})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daebd899",
      "metadata": {},
      "source": [
        "Creates batches (https://huggingface.co/docs/transformers/pad_truncation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "ba29f94d",
      "metadata": {},
      "outputs": [],
      "source": [
        "context_length = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "b39eace4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 31318\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 3047\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True)\n",
        "    \n",
        "    input_batch = []\n",
        "    \n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize, \n",
        "                                      batched=True, \n",
        "                                      remove_columns=raw_datasets[\"train\"].column_names)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "cad15b04-6990-40bc-9c4f-d79a97b29787",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 31318\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 3047\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbb7482",
      "metadata": {},
      "source": [
        "Initiate new Llama with config as wished:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "5fe1190f-7357-47a3-9cc3-00468439c619",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16002"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d589d379",
      "metadata": {},
      "outputs": [],
      "source": [
        "config = LlamaConfig(\n",
        "    vocab_size=len(tokenizer),\n",
        "    hidden_size=128,          # Increased from 16\n",
        "    num_hidden_layers=6,      # Increased from 2\n",
        "    intermediate_size=512,    # Increased from 16\n",
        "    num_attention_heads=8,    # Increased from 2\n",
        "    bos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "    eos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "    pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
        "    max_position_embeddings=512  # Increased from 16\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# config = LlamaConfig(\n",
        "#     vocab_size=len(tokenizer),\n",
        "#     hidden_size=16,\n",
        "#     num_hidden_layers=2,\n",
        "#     intermediate_size=16,\n",
        "#     num_attention_heads=2,\n",
        "#     bos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "#     eos_token_id=tokenizer.convert_tokens_to_ids(\"<|endoftext|>\"),\n",
        "#     pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
        "#     max_position_embeddings=16\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de1e5e3-bea9-4ca2-bbe6-7b414e379fe8",
      "metadata": {},
      "source": [
        "Set seed for weight initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "4c99f330-8a8c-4878-ac9c-9e576e488be6",
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a9ab15-5773-4a81-ba4a-308f389093a1",
      "metadata": {},
      "source": [
        "New model object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "5a07e805",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = LlamaForCausalLM(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "aa89da5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6343c2e-30bc-4e99-ab8d-9a2d3d4c37ae",
      "metadata": {},
      "source": [
        "Check out param size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "89f276d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model num parameters = 5671040\n"
          ]
        }
      ],
      "source": [
        "print(f'model num parameters = {model.num_parameters()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "f2fb20ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_dict = config.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b112127-7368-4fc6-a6a5-e990bbbe625c",
      "metadata": {},
      "source": [
        "Set training parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "22bee44e",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    overwrite_output_dir=True,\n",
        "    save_strategy = \"epoch\", # saves after every epoch\n",
        "    #save_strategy = \"steps\", \n",
        "    #save_steps = 0.1, # if below zero, then saves after every (n*100)% of training steps\n",
        "    save_total_limit=2,  # set to zero to avoid saving\n",
        "    eval_strategy = \"epoch\",\n",
        "    #eval_steps = 0.1,\n",
        "    num_train_epochs= 5,\n",
        "    #max_steps = 1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    warmup_steps=100, \n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=1e-4, #original: 3e-4, # normal: 5e-4\n",
        "    logging_steps=50,\n",
        "    #fp16=True, ## only on CUDA\n",
        "    #load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    #use_mps_device=True, ## only on apple silicon\n",
        "    #use_cpu = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3537432-881c-42ed-83d4-fef5456efe45",
      "metadata": {},
      "source": [
        "Initialize trainer object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "893b3d99",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ln/fhk1_2pd10n5h6m_z3xbszwm0000gn/T/ipykernel_41073/669889199.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets['train'],#[:15000]['input_ids'],\n",
        "    eval_dataset=tokenized_datasets['validation']#[:1200]['input_ids'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d20de00-46b5-478c-a92a-58f0e5433bd8",
      "metadata": {},
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "eca36080",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4890' max='4890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4890/4890 09:46, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5.405300</td>\n",
              "      <td>6.053078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.999200</td>\n",
              "      <td>5.795939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.829700</td>\n",
              "      <td>5.700454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.752200</td>\n",
              "      <td>5.691287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.651700</td>\n",
              "      <td>5.697586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4890, training_loss=5.129633857087367, metrics={'train_runtime': 586.4956, 'train_samples_per_second': 266.993, 'train_steps_per_second': 8.338, 'total_flos': 200692901308416.0, 'train_loss': 5.129633857087367, 'epoch': 4.9992337164750955})"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f7370f-bb06-44a8-b873-5519f5c31cb4",
      "metadata": {},
      "source": [
        "Save logs of losses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "6b00de27-3260-49fd-8865-6980e8bbe5c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(trainer.state.log_history)\n",
        "df.to_csv(model_path+'logs/losses.csv')  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "336a05f8-34d0-402b-a785-0a729d6ebea0",
      "metadata": {},
      "source": [
        "Save final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "f68875c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.save_model(model_path+'final/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1debbe5e-1a7b-49bf-bde3-cd6c23e06608",
      "metadata": {},
      "source": [
        "### Test trained model on text generation\n",
        "\n",
        "With hf pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "a6a24ae6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model_path+'final/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "77720de3",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'From the prison he remembers ) or i was by them to those who be a great other species. that we can have a very that we are to not necessary of the nature in that degree of the state, in a power. it is, in this other the president for every power, and of the members, to find that of the united states, or even to have to produce no few, will not, or we are to probably and if there will be seen in the most of these, are the same states, that to the species of the government, with a single, without the members of the federal powers.'},\n",
              " {'generated_text': 'From the prison he remembers ; that the constitution ought to have not been more to never been so at a country. they, as i have been, from many of the people, when the same. from their species, or a more than that which they have shown from the new york-continued as are a, we do not, that in all our government, as the general whole-water, and not all that they can have been to be it, of the states, and their cases, in most species may be less or, for the same species. hence, at the senate, to me to be some,'},\n",
              " {'generated_text': 'From the prison he remembers  of the government of the same manner on all which is the people may to be to be mores as, i have that it may have been as more a great and not they see it in the subject. i might have been seen we are to be any one. in other. for its laws have been found to be most other the several states, it may be the other number of the great forms a certain species, without the latter. in the number of the same new the whole, if it, be often said to have the united against his same and not of the same government, and'},\n",
              " {'generated_text': 'From the prison he remembers . the same for other subject, as as i be necessary by which it is to can often a few new and great laws, as which or of natural and to our constitution. if so is only much very to the general members of the legislature. we was the whole one of the executive, are, but in the common and as the present, in which the state, there be in all, and the case, under one species the same one states may that they are the very a other, that in which for the national degree of two-operation which not on the whole case, as,'},\n",
              " {'generated_text': 'From the prison he remembers  of least forms; and the subject is not from each other. he will be very have one powers of them. \" with a federal state, and from which the state of the public state, it is so to me, and there with the other which in the case, who to the nature; on them. to the power to take an members, to me, that the executive, the, and by some other part of our nature, that it would have made of such as they has become not, however by some state. but the federal laws, the states, a national or the most'},\n",
              " {'generated_text': 'From the prison he remembers : from our own government under any other states, and would be not more-like cases in the most few, as i is to not, for it of the same or to their laws, and the same new powers of the species to a view the united would be, in these, we were all the plan; would have been the state. it for the same species, the constitution, the state. on the first one of the species to in the whole part of the states, and at the whole. if the other, that we is not to find any other, would be less often'},\n",
              " {'generated_text': 'From the prison he remembers : or and by, and they has in a number of species, and by the common and when the most much at a few other, in other, and of the senate, it is a long-water of the union, which when an proper other plants, all the species of no less to all its state. and in the federal which in the constitution only species and some other members, which this same case, and not in this is a power to the same state; but no that, as it will have have many to have not more to its union-marked and the whole which they'},\n",
              " {'generated_text': 'From the prison he remembers  for every other and of such and man. but the one of the states are, that are only in the present number of the same species from a very more, and more to be, in all time, as a national government, when it is to appear, the public constitution, by any other and the people will have seen is a common constitution should be shown which the same species; and the same period and well; and that of the same species is the different a different and a more, in the united; in the several states, of these one of such much on the constitution in the'},\n",
              " {'generated_text': 'From the prison he remembers  to that of the union, and must have to the same place, would find in the states to the view of the great body. the power of life, in the first the people that the world, which may be the view are the most common species have not, a good, than one states, and of the legislature on that states, on, and that government of the same species of the people. the government can no the most given to be it as one body of each part of the government will be much the power of the same general it is well from the whole, but be,'},\n",
              " {'generated_text': 'From the prison he remembers . but we is found to do no more to which its other states, and by a very one of their power to be no own time. publius, to be with a single, the new and to in an plan, we, when \" in the power of one species or well) and in the legislature at any other forms, to be on the same manner on the species of life, or a very no common, and do so that of such a single one one is, that of their case. but some of a distinct authority of a own union, are the general state, and,'},\n",
              " {'generated_text': 'From the prison he remembers  and extinct for any the plan of the united states; the power of this state. it is the executive, it seems to be by the latter. and thus that in species, they may be from the same of the people, this other, it would be of the executive, it is a very, in the number to have been that they of all of the people for the several state for an public or to a power of the united states from the federal and it would may have, no other; in a view, that from the federal of the state of they was now with their great species'},\n",
              " {'generated_text': 'From the prison he remembers : therefore. though it has been thus formed no most of the different and, than to the present species of the same species from the state. if on the principle of life of the same species, i have they do not, that of a different, in the most more or, which will be the constitution, be of the senate will be the one of the new and new of its part. he must have only states, in, it is not far in our single, and the power to be much, in certain species, we see, with the general states, in the laws of the'},\n",
              " {'generated_text': 'From the prison he remembers . in a the same number of natural selection, of other, and as the state, by the government of the plan of the general other. but these different time, in some species which i are it not a great and to the view in the national state. in a long-thrush of every two and in the same species, when it, a case, in the power of the more from the other powers shall be seen with the same, by the nature of these case, that of it has been a common species by the executive, is not, will be most necessary, the national species'},\n",
              " {'generated_text': 'From the prison he remembers ), and thus not by the latter, the plan of the people. if they has been made in a constitution, it should be not to have been much to be all, that the people, and by any varieties, or which will never be, with that in the other and more. yet all, even as an great. it have been able to give, that that we were not to can be by the union; but it often made on the state, there is by any more than a government, i have been made. in a a greater more than in species of the same a national'},\n",
              " {'generated_text': 'From the prison he remembers , and the same power of all in which not the new york. it is not at that degree in a the national people, but we may be less much more much in a few power of a power of nature, in two-defined the principle of the other government not made are in the species of our cases; and be their public powers at all, that may never be a power of america, even with the power from the power of each of cases, of every government, that government. on a, but have on it, that of life, but there is the plan, how not'},\n",
              " {'generated_text': 'From the prison he remembers s as their varieties in these number, or in any other degree. in some of one manner which would not see in other and of the plan. in the president it is less less well. this has been a number; and i cannot be, as the more a little to have the different,, which a laws be more that of this several of them, but to be to the states, which in the latter, and do not; and on the people. from this same people, in the most a part of the great government may in the species of its power of the laws, in'},\n",
              " {'generated_text': 'From the prison he remembers . as the part of the constitution, and that which believe at the most one species, though we are to had its most other, must have in a number of the people; and it and all the same forms which may have been well as their own new case, there may have taken the nature, must be seen to those, that the state state selection of the several other, to be by them are more for many it at a laws. the power of a manner for the most less of the president, if we become the species than these. but their species, this same power of the'},\n",
              " {'generated_text': 'From the prison he remembers ; and but to be formed or so it the plan of the legislature, on the authority of a great species, in the legislative. the most in an members of the same the whole, and the laws will the view of the states, which, the same constitution, the general. if some of the federal government in the new-water, yet, will not in a large. the other, however, by the government of it they are a state were to require. the several part are very very different on the people. no own number of that as it are it would be at the one'},\n",
              " {'generated_text': 'From the prison he remembers  of the latter, he is, if, in the government, and are to be the national of the new. in great authority; that it is, that this may be a good species, though very necessary for they can be to be said only, by the same states of the federal body of its different states; in my different great power of the first in the most most, with an union, no government or that would to be called. an same species; and it is from the latter, we may it, to require each of the members of the most to these states a great case'},\n",
              " {'generated_text': 'From the prison he remembers . hence, in its right in a general most, is the executive a a great species the same power of life. to a other number of a common varieties is by a more time in many species. this state of this national same different and their own cases of that we are of a national of the principle, in a long-marked or the constitution-like more less that of the state, and there has been most different species of any one most much the state, in this to any cases of the subject may be able to no first time, we believe that, as well- early part'}]"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe(\"From the prison he remembers \", do_sample = True, \n",
        "     num_return_sequences = 20, \n",
        "     max_length=128\n",
        "     #top_k=50,\n",
        "     #top_p=0.8,\n",
        "     #temperature=1.0,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f91eb39a-4197-4aa7-a775-5596a366361c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

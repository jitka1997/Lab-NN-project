{
  "best_global_step": 4596,
  "best_metric": 5.392587661743164,
  "best_model_checkpoint": "/Users/jitkamuravska/Neural-Networks/project/fiction-model/checkpoint-4596",
  "epoch": 4.9962987154365335,
  "eval_steps": 500,
  "global_step": 5740,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.043544524276072284,
      "grad_norm": 1.7953773736953735,
      "learning_rate": 4.9e-05,
      "loss": 9.031,
      "step": 50
    },
    {
      "epoch": 0.08708904855214457,
      "grad_norm": 1.2690573930740356,
      "learning_rate": 9.900000000000001e-05,
      "loss": 8.5276,
      "step": 100
    },
    {
      "epoch": 0.13063357282821686,
      "grad_norm": 1.19159734249115,
      "learning_rate": 9.998137712428306e-05,
      "loss": 7.7458,
      "step": 150
    },
    {
      "epoch": 0.17417809710428914,
      "grad_norm": 1.1697934865951538,
      "learning_rate": 9.992399505025667e-05,
      "loss": 7.131,
      "step": 200
    },
    {
      "epoch": 0.21772262138036141,
      "grad_norm": 1.0105807781219482,
      "learning_rate": 9.982789052828251e-05,
      "loss": 6.668,
      "step": 250
    },
    {
      "epoch": 0.2612671456564337,
      "grad_norm": 0.8813264966011047,
      "learning_rate": 9.96931380997178e-05,
      "loss": 6.348,
      "step": 300
    },
    {
      "epoch": 0.30481166993250597,
      "grad_norm": 0.6495028734207153,
      "learning_rate": 9.95198422823179e-05,
      "loss": 6.1659,
      "step": 350
    },
    {
      "epoch": 0.3483561942085783,
      "grad_norm": 0.7102535963058472,
      "learning_rate": 9.930813748916939e-05,
      "loss": 6.1003,
      "step": 400
    },
    {
      "epoch": 0.3919007184846506,
      "grad_norm": 0.7622928619384766,
      "learning_rate": 9.90581879244355e-05,
      "loss": 6.0203,
      "step": 450
    },
    {
      "epoch": 0.43544524276072283,
      "grad_norm": 0.9551051259040833,
      "learning_rate": 9.877018745599482e-05,
      "loss": 5.9622,
      "step": 500
    },
    {
      "epoch": 0.47898976703679513,
      "grad_norm": 0.8563610315322876,
      "learning_rate": 9.844435946507187e-05,
      "loss": 5.903,
      "step": 550
    },
    {
      "epoch": 0.5225342913128674,
      "grad_norm": 0.8430218696594238,
      "learning_rate": 9.808095667297642e-05,
      "loss": 5.8138,
      "step": 600
    },
    {
      "epoch": 0.5660788155889397,
      "grad_norm": 1.0633270740509033,
      "learning_rate": 9.768026094508578e-05,
      "loss": 5.7809,
      "step": 650
    },
    {
      "epoch": 0.6096233398650119,
      "grad_norm": 1.183131217956543,
      "learning_rate": 9.72425830722221e-05,
      "loss": 5.7273,
      "step": 700
    },
    {
      "epoch": 0.6531678641410843,
      "grad_norm": 1.0901756286621094,
      "learning_rate": 9.676826252959427e-05,
      "loss": 5.6896,
      "step": 750
    },
    {
      "epoch": 0.6967123884171565,
      "grad_norm": 1.1637526750564575,
      "learning_rate": 9.625766721349155e-05,
      "loss": 5.6574,
      "step": 800
    },
    {
      "epoch": 0.7402569126932288,
      "grad_norm": 1.235186219215393,
      "learning_rate": 9.57111931559328e-05,
      "loss": 5.6308,
      "step": 850
    },
    {
      "epoch": 0.7838014369693012,
      "grad_norm": 1.3128260374069214,
      "learning_rate": 9.512926421749304e-05,
      "loss": 5.5881,
      "step": 900
    },
    {
      "epoch": 0.8273459612453734,
      "grad_norm": 1.2227826118469238,
      "learning_rate": 9.451233175854526e-05,
      "loss": 5.5312,
      "step": 950
    },
    {
      "epoch": 0.8708904855214457,
      "grad_norm": 1.2962497472763062,
      "learning_rate": 9.386087428917274e-05,
      "loss": 5.4934,
      "step": 1000
    },
    {
      "epoch": 0.914435009797518,
      "grad_norm": 1.1600360870361328,
      "learning_rate": 9.317539709802322e-05,
      "loss": 5.5065,
      "step": 1050
    },
    {
      "epoch": 0.9579795340735903,
      "grad_norm": 1.468061089515686,
      "learning_rate": 9.245643186039285e-05,
      "loss": 5.4394,
      "step": 1100
    },
    {
      "epoch": 1.0,
      "eval_loss": 5.8794379234313965,
      "eval_runtime": 5.0039,
      "eval_samples_per_second": 746.016,
      "eval_steps_per_second": 93.327,
      "step": 1149
    },
    {
      "epoch": 1.0008708904855215,
      "grad_norm": 1.283307671546936,
      "learning_rate": 9.170453622584396e-05,
      "loss": 5.4508,
      "step": 1150
    },
    {
      "epoch": 1.0444154147615938,
      "grad_norm": 1.360396385192871,
      "learning_rate": 9.092029338567652e-05,
      "loss": 5.3564,
      "step": 1200
    },
    {
      "epoch": 1.087959939037666,
      "grad_norm": 1.5770574808120728,
      "learning_rate": 9.010431162058848e-05,
      "loss": 5.3089,
      "step": 1250
    },
    {
      "epoch": 1.1315044633137383,
      "grad_norm": 1.4734289646148682,
      "learning_rate": 8.925722382887647e-05,
      "loss": 5.2889,
      "step": 1300
    },
    {
      "epoch": 1.1750489875898107,
      "grad_norm": 1.5741453170776367,
      "learning_rate": 8.837968703554193e-05,
      "loss": 5.2682,
      "step": 1350
    },
    {
      "epoch": 1.2185935118658828,
      "grad_norm": 1.803412914276123,
      "learning_rate": 8.747238188268427e-05,
      "loss": 5.2839,
      "step": 1400
    },
    {
      "epoch": 1.2621380361419552,
      "grad_norm": 1.6829378604888916,
      "learning_rate": 8.65360121015758e-05,
      "loss": 5.2209,
      "step": 1450
    },
    {
      "epoch": 1.3056825604180275,
      "grad_norm": 1.8334470987319946,
      "learning_rate": 8.5571303966828e-05,
      "loss": 5.2569,
      "step": 1500
    },
    {
      "epoch": 1.3492270846940997,
      "grad_norm": 1.7327646017074585,
      "learning_rate": 8.457900573307263e-05,
      "loss": 5.2,
      "step": 1550
    },
    {
      "epoch": 1.392771608970172,
      "grad_norm": 1.7961832284927368,
      "learning_rate": 8.355988705459448e-05,
      "loss": 5.1846,
      "step": 1600
    },
    {
      "epoch": 1.4363161332462444,
      "grad_norm": 1.7055909633636475,
      "learning_rate": 8.251473838836602e-05,
      "loss": 5.1916,
      "step": 1650
    },
    {
      "epoch": 1.4798606575223165,
      "grad_norm": 1.9577205181121826,
      "learning_rate": 8.144437038094681e-05,
      "loss": 5.2106,
      "step": 1700
    },
    {
      "epoch": 1.523405181798389,
      "grad_norm": 1.9227101802825928,
      "learning_rate": 8.034961323972326e-05,
      "loss": 5.1461,
      "step": 1750
    },
    {
      "epoch": 1.5669497060744613,
      "grad_norm": 2.0307350158691406,
      "learning_rate": 7.923131608897668e-05,
      "loss": 5.1634,
      "step": 1800
    },
    {
      "epoch": 1.6104942303505334,
      "grad_norm": 1.9466346502304077,
      "learning_rate": 7.809034631127868e-05,
      "loss": 5.1387,
      "step": 1850
    },
    {
      "epoch": 1.6540387546266055,
      "grad_norm": 1.9610488414764404,
      "learning_rate": 7.692758887472484e-05,
      "loss": 5.1291,
      "step": 1900
    },
    {
      "epoch": 1.6975832789026781,
      "grad_norm": 1.9934433698654175,
      "learning_rate": 7.574394564652893e-05,
      "loss": 5.1093,
      "step": 1950
    },
    {
      "epoch": 1.7411278031787503,
      "grad_norm": 2.4901840686798096,
      "learning_rate": 7.454033469350922e-05,
      "loss": 5.1087,
      "step": 2000
    },
    {
      "epoch": 1.7846723274548224,
      "grad_norm": 2.0041043758392334,
      "learning_rate": 7.33176895700103e-05,
      "loss": 5.0653,
      "step": 2050
    },
    {
      "epoch": 1.828216851730895,
      "grad_norm": 1.9965695142745972,
      "learning_rate": 7.20769585938121e-05,
      "loss": 5.0895,
      "step": 2100
    },
    {
      "epoch": 1.8717613760069671,
      "grad_norm": 1.873496174812317,
      "learning_rate": 7.081910411058821e-05,
      "loss": 5.0876,
      "step": 2150
    },
    {
      "epoch": 1.9153059002830393,
      "grad_norm": 2.093864679336548,
      "learning_rate": 6.954510174748351e-05,
      "loss": 5.0685,
      "step": 2200
    },
    {
      "epoch": 1.9588504245591118,
      "grad_norm": 2.6973092555999756,
      "learning_rate": 6.825593965639053e-05,
      "loss": 5.0822,
      "step": 2250
    },
    {
      "epoch": 2.0,
      "eval_loss": 5.5311737060546875,
      "eval_runtime": 4.8344,
      "eval_samples_per_second": 772.177,
      "eval_steps_per_second": 96.6,
      "step": 2298
    },
    {
      "epoch": 2.001741780971043,
      "grad_norm": 1.9838523864746094,
      "learning_rate": 6.695261774751108e-05,
      "loss": 5.0469,
      "step": 2300
    },
    {
      "epoch": 2.045286305247115,
      "grad_norm": 2.094346046447754,
      "learning_rate": 6.563614691379811e-05,
      "loss": 4.935,
      "step": 2350
    },
    {
      "epoch": 2.0888308295231877,
      "grad_norm": 2.3464035987854004,
      "learning_rate": 6.430754824687859e-05,
      "loss": 4.9308,
      "step": 2400
    },
    {
      "epoch": 2.13237535379926,
      "grad_norm": 2.5234720706939697,
      "learning_rate": 6.296785224506645e-05,
      "loss": 4.913,
      "step": 2450
    },
    {
      "epoch": 2.175919878075332,
      "grad_norm": 2.4873669147491455,
      "learning_rate": 6.161809801407919e-05,
      "loss": 4.9336,
      "step": 2500
    },
    {
      "epoch": 2.219464402351404,
      "grad_norm": 2.3910210132598877,
      "learning_rate": 6.0259332461078334e-05,
      "loss": 4.9073,
      "step": 2550
    },
    {
      "epoch": 2.2630089266274767,
      "grad_norm": 2.6023294925689697,
      "learning_rate": 5.8892609482659035e-05,
      "loss": 4.917,
      "step": 2600
    },
    {
      "epoch": 2.306553450903549,
      "grad_norm": 2.485126495361328,
      "learning_rate": 5.751898914741831e-05,
      "loss": 4.9142,
      "step": 2650
    },
    {
      "epoch": 2.3500979751796214,
      "grad_norm": 2.3412978649139404,
      "learning_rate": 5.613953687373622e-05,
      "loss": 4.9028,
      "step": 2700
    },
    {
      "epoch": 2.3936424994556935,
      "grad_norm": 2.505117416381836,
      "learning_rate": 5.4755322603407475e-05,
      "loss": 4.8455,
      "step": 2750
    },
    {
      "epoch": 2.4371870237317657,
      "grad_norm": 2.4420418739318848,
      "learning_rate": 5.336741997176471e-05,
      "loss": 4.9155,
      "step": 2800
    },
    {
      "epoch": 2.480731548007838,
      "grad_norm": 2.4449222087860107,
      "learning_rate": 5.197690547493679e-05,
      "loss": 4.8991,
      "step": 2850
    },
    {
      "epoch": 2.5242760722839104,
      "grad_norm": 2.4890432357788086,
      "learning_rate": 5.058485763488836e-05,
      "loss": 4.886,
      "step": 2900
    },
    {
      "epoch": 2.5678205965599825,
      "grad_norm": 2.3094377517700195,
      "learning_rate": 4.919235616288798e-05,
      "loss": 4.8514,
      "step": 2950
    },
    {
      "epoch": 2.611365120836055,
      "grad_norm": 2.47814679145813,
      "learning_rate": 4.7800481122053855e-05,
      "loss": 4.8716,
      "step": 3000
    },
    {
      "epoch": 2.6549096451121272,
      "grad_norm": 2.5476887226104736,
      "learning_rate": 4.641031208962662e-05,
      "loss": 4.8497,
      "step": 3050
    },
    {
      "epoch": 2.6984541693881994,
      "grad_norm": 2.3745217323303223,
      "learning_rate": 4.502292731961906e-05,
      "loss": 4.8721,
      "step": 3100
    },
    {
      "epoch": 2.7419986936642715,
      "grad_norm": 2.5460739135742188,
      "learning_rate": 4.363940290649206e-05,
      "loss": 4.8627,
      "step": 3150
    },
    {
      "epoch": 2.785543217940344,
      "grad_norm": 2.5876784324645996,
      "learning_rate": 4.226081195050571e-05,
      "loss": 4.8658,
      "step": 3200
    },
    {
      "epoch": 2.8290877422164162,
      "grad_norm": 2.583108901977539,
      "learning_rate": 4.088822372539263e-05,
      "loss": 4.8652,
      "step": 3250
    },
    {
      "epoch": 2.872632266492489,
      "grad_norm": 2.7960963249206543,
      "learning_rate": 3.952270284899934e-05,
      "loss": 4.8664,
      "step": 3300
    },
    {
      "epoch": 2.916176790768561,
      "grad_norm": 2.6989433765411377,
      "learning_rate": 3.816530845753889e-05,
      "loss": 4.8339,
      "step": 3350
    },
    {
      "epoch": 2.959721315044633,
      "grad_norm": 2.7255289554595947,
      "learning_rate": 3.6817093384095145e-05,
      "loss": 4.8703,
      "step": 3400
    },
    {
      "epoch": 3.0,
      "eval_loss": 5.420082092285156,
      "eval_runtime": 4.6852,
      "eval_samples_per_second": 796.767,
      "eval_steps_per_second": 99.676,
      "step": 3447
    },
    {
      "epoch": 3.002612671456564,
      "grad_norm": 2.495488166809082,
      "learning_rate": 3.5479103342016e-05,
      "loss": 4.8587,
      "step": 3450
    },
    {
      "epoch": 3.0461571957326368,
      "grad_norm": 2.4569075107574463,
      "learning_rate": 3.415237611382879e-05,
      "loss": 4.7582,
      "step": 3500
    },
    {
      "epoch": 3.089701720008709,
      "grad_norm": 3.063530921936035,
      "learning_rate": 3.28379407463072e-05,
      "loss": 4.7601,
      "step": 3550
    },
    {
      "epoch": 3.133246244284781,
      "grad_norm": 2.483560562133789,
      "learning_rate": 3.1536816752313694e-05,
      "loss": 4.7229,
      "step": 3600
    },
    {
      "epoch": 3.1767907685608536,
      "grad_norm": 2.8543519973754883,
      "learning_rate": 3.025001332003683e-05,
      "loss": 4.7566,
      "step": 3650
    },
    {
      "epoch": 3.2203352928369258,
      "grad_norm": 2.9236369132995605,
      "learning_rate": 2.8978528530236693e-05,
      "loss": 4.7625,
      "step": 3700
    },
    {
      "epoch": 3.263879817112998,
      "grad_norm": 2.777052402496338,
      "learning_rate": 2.7723348582105457e-05,
      "loss": 4.765,
      "step": 3750
    },
    {
      "epoch": 3.3074243413890705,
      "grad_norm": 2.746086597442627,
      "learning_rate": 2.648544702834372e-05,
      "loss": 4.7647,
      "step": 3800
    },
    {
      "epoch": 3.3509688656651426,
      "grad_norm": 3.0362794399261475,
      "learning_rate": 2.5265784020045835e-05,
      "loss": 4.7337,
      "step": 3850
    },
    {
      "epoch": 3.3945133899412148,
      "grad_norm": 3.261636972427368,
      "learning_rate": 2.406530556197971e-05,
      "loss": 4.736,
      "step": 3900
    },
    {
      "epoch": 3.4380579142172873,
      "grad_norm": 3.0990614891052246,
      "learning_rate": 2.2884942778839202e-05,
      "loss": 4.7327,
      "step": 3950
    },
    {
      "epoch": 3.4816024384933595,
      "grad_norm": 3.0111753940582275,
      "learning_rate": 2.1725611193037553e-05,
      "loss": 4.7344,
      "step": 4000
    },
    {
      "epoch": 3.5251469627694316,
      "grad_norm": 3.0812180042266846,
      "learning_rate": 2.0588210014602778e-05,
      "loss": 4.7215,
      "step": 4050
    },
    {
      "epoch": 3.5686914870455038,
      "grad_norm": 2.967916965484619,
      "learning_rate": 1.9473621443725033e-05,
      "loss": 4.6784,
      "step": 4100
    },
    {
      "epoch": 3.6122360113215763,
      "grad_norm": 2.9630606174468994,
      "learning_rate": 1.8382709986497536e-05,
      "loss": 4.7193,
      "step": 4150
    },
    {
      "epoch": 3.6557805355976485,
      "grad_norm": 3.2411916255950928,
      "learning_rate": 1.7316321784381423e-05,
      "loss": 4.7387,
      "step": 4200
    },
    {
      "epoch": 3.699325059873721,
      "grad_norm": 3.174328565597534,
      "learning_rate": 1.6275283957914778e-05,
      "loss": 4.7088,
      "step": 4250
    },
    {
      "epoch": 3.742869584149793,
      "grad_norm": 3.2119851112365723,
      "learning_rate": 1.526040396517477e-05,
      "loss": 4.7485,
      "step": 4300
    },
    {
      "epoch": 3.7864141084258653,
      "grad_norm": 3.203772783279419,
      "learning_rate": 1.4272468975490532e-05,
      "loss": 4.6945,
      "step": 4350
    },
    {
      "epoch": 3.8299586327019375,
      "grad_norm": 2.988250255584717,
      "learning_rate": 1.3312245258892631e-05,
      "loss": 4.7396,
      "step": 4400
    },
    {
      "epoch": 3.87350315697801,
      "grad_norm": 3.104753017425537,
      "learning_rate": 1.2380477591772616e-05,
      "loss": 4.7221,
      "step": 4450
    },
    {
      "epoch": 3.917047681254082,
      "grad_norm": 2.9815375804901123,
      "learning_rate": 1.147788867921354e-05,
      "loss": 4.7189,
      "step": 4500
    },
    {
      "epoch": 3.960592205530155,
      "grad_norm": 3.185906171798706,
      "learning_rate": 1.0605178594439685e-05,
      "loss": 4.7059,
      "step": 4550
    },
    {
      "epoch": 4.0,
      "eval_loss": 5.392587661743164,
      "eval_runtime": 4.7287,
      "eval_samples_per_second": 789.438,
      "eval_steps_per_second": 98.759,
      "step": 4596
    },
    {
      "epoch": 4.003483561942086,
      "grad_norm": 3.2718076705932617,
      "learning_rate": 9.763024235820229e-06,
      "loss": 4.7302,
      "step": 4600
    },
    {
      "epoch": 4.047028086218158,
      "grad_norm": 2.947813034057617,
      "learning_rate": 8.952078801847858e-06,
      "loss": 4.6454,
      "step": 4650
    },
    {
      "epoch": 4.09057261049423,
      "grad_norm": 3.050579309463501,
      "learning_rate": 8.172971284499743e-06,
      "loss": 4.6801,
      "step": 4700
    },
    {
      "epoch": 4.134117134770302,
      "grad_norm": 3.055532932281494,
      "learning_rate": 7.426305981373782e-06,
      "loss": 4.6706,
      "step": 4750
    },
    {
      "epoch": 4.177661659046375,
      "grad_norm": 3.3043622970581055,
      "learning_rate": 6.712662026978444e-06,
      "loss": 4.6572,
      "step": 4800
    },
    {
      "epoch": 4.2212061833224475,
      "grad_norm": 3.172337055206299,
      "learning_rate": 6.032592943539878e-06,
      "loss": 4.6736,
      "step": 4850
    },
    {
      "epoch": 4.26475070759852,
      "grad_norm": 3.4607911109924316,
      "learning_rate": 5.386626211674506e-06,
      "loss": 4.696,
      "step": 4900
    },
    {
      "epoch": 4.308295231874592,
      "grad_norm": 3.263037919998169,
      "learning_rate": 4.775262861260355e-06,
      "loss": 4.6998,
      "step": 4950
    },
    {
      "epoch": 4.351839756150664,
      "grad_norm": 3.374929904937744,
      "learning_rate": 4.198977082824252e-06,
      "loss": 4.6776,
      "step": 5000
    },
    {
      "epoch": 4.395384280426736,
      "grad_norm": 3.2594563961029053,
      "learning_rate": 3.65821585974635e-06,
      "loss": 4.6751,
      "step": 5050
    },
    {
      "epoch": 4.438928804702808,
      "grad_norm": 3.30271053314209,
      "learning_rate": 3.153398621567327e-06,
      "loss": 4.6427,
      "step": 5100
    },
    {
      "epoch": 4.482473328978881,
      "grad_norm": 2.9125869274139404,
      "learning_rate": 2.6849169186670965e-06,
      "loss": 4.6525,
      "step": 5150
    },
    {
      "epoch": 4.526017853254953,
      "grad_norm": 3.112624406814575,
      "learning_rate": 2.2531341185673515e-06,
      "loss": 4.6493,
      "step": 5200
    },
    {
      "epoch": 4.5695623775310255,
      "grad_norm": 3.1251380443573,
      "learning_rate": 1.8583851240935869e-06,
      "loss": 4.6601,
      "step": 5250
    },
    {
      "epoch": 4.613106901807098,
      "grad_norm": 4.234625339508057,
      "learning_rate": 1.500976113615049e-06,
      "loss": 4.6703,
      "step": 5300
    },
    {
      "epoch": 4.65665142608317,
      "grad_norm": 3.4115116596221924,
      "learning_rate": 1.1811843035642367e-06,
      "loss": 4.6515,
      "step": 5350
    },
    {
      "epoch": 4.700195950359243,
      "grad_norm": 3.2714743614196777,
      "learning_rate": 8.992577334201002e-07,
      "loss": 4.6868,
      "step": 5400
    },
    {
      "epoch": 4.743740474635315,
      "grad_norm": 3.2098159790039062,
      "learning_rate": 6.554150733216413e-07,
      "loss": 4.652,
      "step": 5450
    },
    {
      "epoch": 4.787284998911387,
      "grad_norm": 3.2660953998565674,
      "learning_rate": 4.498454544612618e-07,
      "loss": 4.6631,
      "step": 5500
    },
    {
      "epoch": 4.830829523187459,
      "grad_norm": 3.0147695541381836,
      "learning_rate": 2.827083223892968e-07,
      "loss": 4.6443,
      "step": 5550
    },
    {
      "epoch": 4.874374047463531,
      "grad_norm": 3.2778267860412598,
      "learning_rate": 1.5413331334360182e-07,
      "loss": 4.6622,
      "step": 5600
    },
    {
      "epoch": 4.9179185717396035,
      "grad_norm": 3.2895381450653076,
      "learning_rate": 6.422015370003975e-08,
      "loss": 4.6483,
      "step": 5650
    },
    {
      "epoch": 4.961463096015676,
      "grad_norm": 3.4510035514831543,
      "learning_rate": 1.303858262192703e-08,
      "loss": 4.6535,
      "step": 5700
    },
    {
      "epoch": 4.9962987154365335,
      "eval_loss": 5.393315315246582,
      "eval_runtime": 4.7864,
      "eval_samples_per_second": 779.917,
      "eval_steps_per_second": 97.568,
      "step": 5740
    }
  ],
  "logging_steps": 50,
  "max_steps": 5740,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 244116054048768.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
